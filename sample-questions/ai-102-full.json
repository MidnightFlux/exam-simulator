[
  {
    "question_number": 1,
    "question_text": "Your organization is developing an AI-powered application using Azure OpenAI in Foundry Models.\nYou need to choose a model for text generation.\nWhat should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Vision in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Code-Davinci-003",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "DALL-E",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "GPT-4",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Azure Vision in Foundry Tools is designed for analyzing visual content and does not support text generation, making it unsuitable for the scenario. Code-Davinci-003 is optimized for code generation tasks and lacks the capabilities required for text generation, which disqualifies it as a viable option. DALL-E specializes in generating images from textual descriptions and does not meet the requirement for text generation. GPT-4, on the other hand, is specifically designed for generating detailed and contextually accurate text responses, making it the most appropriate choice for the given requirement.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/3-azure-ai-services",
      "https://learn.microsoft.com/en-us/training/modules/get-started-openai/2-access-azure-openai"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 2,
    "question_text": "You have an Azure OpenAI in Foundry Models solution. The solution uses a specific GPT-3.5-Turbo model version that was current during initial deployment. Auto-update is disabled.\nSometime later, you investigate the deployed solution and discover that it uses a newer version of the model.\nWhy was the model version updated?",
    "options": [
      {
        "letter": "a",
        "text": "Auto-update is always enabled.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Auto-update is enabled automatically when a new version is released.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Model versions are updated automatically when the version is older than five version updates.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "The model version reached its retirement date.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "As your use of Azure OpenAI evolves, and you start to build and integrate with applications, you might want to manually control model updates so that you can first test and validate whether model performance remains consistent for a use case before performing an upgrade.\nWhen you select a specific model version for a deployment, this version will remain selected until you either choose to manually update it, or once you reach the retirement date of the model. When the retirement date is reached, the model will upgrade to the default version automatically at the time of retirement.",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/openai/how-to/working-with-models?tabs=powershell"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 3,
    "question_text": "Your organization plans to deploy a generative AI solution using Azure OpenAI in Foundry Models with GPT-4 for natural language responses.\nYou need to ensure GPT-4 is available for inferencing via an endpoint.\nWhich three actions should you perform to achieve this goal? Each correct answer presents part of the solution.",
    "options": [
      {
        "letter": "a",
        "text": "Create a new Azure subscription.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Deploy a GPT-4 model",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Provision a Microsoft Foundry resource.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Select GPT-4 from the catalog.",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "Set up a virtual machine.",
        "is_correct": false
      },
      {
        "letter": "f",
        "text": "Use DALL-E.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b",
      "c",
      "d"
    ],
    "explanation": "To deploy GPT-4 for inferencing, provisioning Microsoft Foundry resource is essential to establish the infrastructure. Selecting GPT-4 from the catalog ensures the correct model is chosen for the intended use case. Deploying GPT-4 to an endpoint makes it accessible for inferencing, completing the deployment process. Setting up a virtual machine is unnecessary because Microsoft Foundry uses managed endpoints. Creating a new Azure subscription is irrelevant if an existing subscription is available. Using DALL-E is incorrect because it is designed for image generation rather than natural language processing.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/explore-models-azure-ai-studio/3-deploy-model",
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/4-azure-ai-foundry"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 4,
    "question_text": "An organization plans to use Azure OpenAI in Foundry Models to process user input and generate content.\nYou need to create visual content from text descriptions.\nWhat should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Azure AI Search",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "DALL-E",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "GPT-4",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Text Embedding Models",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "DALL-E is the appropriate choice for creating visual content from text descriptions, as it is specifically designed for this purpose. Azure Cognitive Search focuses on information retrieval and does not support visual content creation. GPT-4 is optimized for text-based tasks and lacks the capability to generate visual content. Text Embedding Models are intended for embedding tasks and are unrelated to the goal of creating visual content from text descriptions.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/get-started-openai/3-use-azure-openai-studio",
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/5-tools-and-sdks"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 5,
    "question_text": "Your company uses Azure OpenAI in Foundry Models to analyze large datasets.\nYou need to configure the application to retrieve relevant data efficiently.\nWhat action should you take?",
    "options": [
      {
        "letter": "a",
        "text": "Deploy a GPT-4 model.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Enable vector search.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Increase chunk size.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use keyword-based search.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "Enabling vector search enhances the relevance and efficiency of data retrieval, meeting the stated goal. Deploying an additional model does not address the need for efficient data retrieval, making it unsuitable. Increasing chunk size affects data processing but does not directly improve retrieval efficiency. Using keyword-based search lacks semantic capabilities, which are critical for achieving the desired outcome.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/build-copilot-ai-studio/3-search-data",
      "https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/use-your-data?tabs=ai-search%2Ccopilot"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 6,
    "question_text": "You are building a GPT-based chat application that will answer questions about your company.\nYou plan to use the Using your data feature in Microsoft Foundry to ground the model with company data.\nWhich four types of files can you use to ground the model? Each correct answer presents a complete solution.",
    "options": [
      {
        "letter": "a",
        "text": "HTML",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "MD",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "PDF",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "TXT",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "XML",
        "is_correct": false
      },
      {
        "letter": "f",
        "text": "ZIP",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a",
      "b",
      "c",
      "d"
    ],
    "explanation": "Currently only TXT, MD, HTML, PDF, Microsoft Word, and PowerPoint files can be used and are supported using the “Using your data” feature in Azure OpenAI. ZIP and XML files are not supported.",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data?tabs=ai-search"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 7,
    "question_text": "You are building a knowledge mining solution by using Azure AI Search.\nYou need to ensure that the solution supports wildcard queries in search requests.\nWhat should you include in the REST API request?",
    "options": [
      {
        "letter": "a",
        "text": "“queryType”: “extended”",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "“queryType”: “full”",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "“queryType”: “simple”",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "“queryType”: “wildcard”",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "queryType “full” extends the default Simple query language by adding support for more operators and query types, such as wildcard, fuzzy, regex, and field-scoped queries.",
    "references": [
      "https://learn.microsoft.com/azure/search/search-lucene-query-architecture",
      "https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 8,
    "question_text": "You have a web app named App1 that performs customs searches.\nYou are building a solution that uses Azure AI Search.\nYou need to include App1 as a custom skill as part of the solution.\nWhich @odata.type should you use to call App1?",
    "options": [
      {
        "letter": "a",
        "text": "Microsoft.Skills.Custom.AmlSkill",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Microsoft.Skills.Custom.WebApiSkill",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Microsoft.Skills.Text.CustomEntityLookupSkill",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Microsoft.Skills.Util.ConditionalSkill",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "Microsoft.Skills.Custom.WebApiSkill allows the extensibility of an AI enrichment pipeline by making an HTTP call to a custom web API.",
    "references": [
      "https://learn.microsoft.com/azure/search/cognitive-search-predefined-skills",
      "https://learn.microsoft.com/training/modules/create-enrichment-pipeline-azure-cognitive-search/"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 9,
    "question_text": "You are building a knowledge mining solution that uses Azure AI Search.\nYou need to apply AI enrichment to your indexer pipeline to generate links to Wikipedia articles.\nWhich skill should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Microsoft.Skills.Custom.WebApiSkill",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Microsoft.Skills.Text.KeyPhraseExtractionSkill",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Microsoft.Skills.Text.PIIDetectionSkill",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Microsoft.Skills.Text.V3.EntityLinkingSkill",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Microsoft.Skills.Text.V3.EntityLinkingSkill uses a pretrained model to generate links for recognized entities to articles in Wikipedia.",
    "references": [
      "https://learn.microsoft.com/azure/search/cognitive-search-predefined-skills",
      "https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 10,
    "question_text": "Your company collects cards during networking events and needs to automate processing to store contact information in a database.\nYou need to extract contact details such as names, email addresses, and phone numbers from the business cards.\nWhat should you use to achieve this goal?",
    "options": [
      {
        "letter": "a",
        "text": "Train a custom model in Azure Document Intelligence in Foundry Tools Studio.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Use Azure Vision in Foundry Tools OCR.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Use the Business Card model in Azure Document Intelligence in Foundry Tools.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Use the ID Document model in Azure Document Intelligence in Foundry Tools.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "The Business Card model in Azure Document Intelligence in Foundry Tools is the optimal choice for processing business cards due to its specialized capabilities for extracting structured data such as names, email addresses, and phone numbers. Training a custom model is unnecessary because the Business Card model already meets the requirements. Azure Vision in Foundry Tools OCR lacks the ability to extract structured data, making it unsuitable for this task. The ID Document model is designed for identity documents and does not provide the necessary functionality for processing business cards.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/9-form-recognizer-studio",
      "https://learn.microsoft.com/en-us/training/modules/use-prebuilt-form-recognizer-models/4-use-financial-id-tax-models"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 11,
    "question_text": "You have an app named App1 that extracts invoice data from PDF files by using an S0 instance of Azure Document Intelligence in Foundry Tools. The PDF files are up to 2 MB each and contain up to 10 pages.\nUsers report that App1 is unable to process some invoices.\nYou need to troubleshoot the issue.\nWhat is a possible cause of the issue?",
    "options": [
      {
        "letter": "a",
        "text": "Some of the files are password protected.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Some of the files are too large.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Some of the files have too many pages.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "The tier of the Azure Document Intelligence in Foundry Tools instance is insufficient.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "The service cannot process password-protected files, and this can cause the service a processing failure for some of the files. Although file size and number of pages can cause failures, the limit for the S0 tier is 500 MB and 2,000 pages.\nThe S0 tier is sufficient for the file characteristics mentioned.",
    "references": [
      "https://learn.microsoft.com/azure/applied-ai-services/form-recognizer/concept-invoice?view=form-recog-3.0.0",
      "https://learn.microsoft.com/training/modules/work-form-recognizer/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 12,
    "question_text": "Your company receives multi-page PDF documents from clients containing specifications and statements of work, often including tables and selection marks.\nYou need to extract text, tables, and selection marks while maintaining the document's structure.\nWhat should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Vision in Foundry Tools OCR (Optical Character Recognition)",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Azure Document Intelligence general model",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure Document Intelligence layout model",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Azure Document Intelligence read model",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Azure Document Intelligence layout model is the most appropriate solution because it is specifically designed to extract text, tables, and selection marks while preserving the document's structure, which is essential for processing multi-page PDF documents. Azure Vision in Foundry Tools OCR is limited to text extraction from images and does not support advanced features like table or selection mark recognition. The general model supports entity extraction but does not focus on maintaining the document's structure. The read model is limited to extracting printed and handwritten text and does not include the advanced capabilities required for this scenario.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/2-what-form-recognizer",
      "https://learn.microsoft.com/en-us/training/modules/use-prebuilt-form-recognizer-models/3-use-general-document-read-layout-models"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 13,
    "question_text": "An organization manually processes a high volume of invoices daily, leading to errors in data extraction.\nYou need to use Microsoft Azure Document Intelligence in Foundry Tools to automate invoice data extraction and integration.\nEach correct answer presents part of the solution. Which two actions should you take?",
    "options": [
      {
        "letter": "a",
        "text": "Integrate the extracted data with an organization's database.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Train a custom model for document processing.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Use a general-purpose model for text extraction.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use the prebuilt Invoice model for data extraction.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "a",
      "d"
    ],
    "explanation": "Integrating the extracted data with an organization's database ensures the information is stored and used effectively within existing systems, supporting downstream processes and workflows. Using the prebuilt Invoice model for data extraction is ideal because it is specifically designed to handle invoice documents, extracting structured data such as key-value pairs and tables efficiently. Training a custom model for document processing is unnecessary in this scenario since the prebuilt Invoice model meets the requirements for standard invoices. Using a general-purpose model for text extraction does not align with the goal of extracting structured data from invoices, as it lacks the specificity required for this task.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/2-what-form-recognizer",
      "https://learn.microsoft.com/en-us/training/modules/plan-form-recognizer-solution/2-understand"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 14,
    "question_text": "Your company receives customer feedback in audio format, including conversations between agents and customers.\nYou need to perform transcription, sentiment analysis, and generate summaries from these recordings.\nWhat service should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Content Understanding in Foundry Tools",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Azure Speech in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure Language in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Azure Document Intelligence in Foundry Tools",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Azure Content Understanding in Foundry Tools is the most suitable solution as it offers transcription, sentiment classification, and summary generation capabilities for audio recordings, addressing all requirements of the scenario. Azure Document Intelligence in Foundry Tools is not appropriate because it processes text-based documents and lacks audio-related functionalities. Azure Language in Foundry Tools offers sentiment analysis and summaries but is unable to transcribe audio. Azure Speech in Foundry Tools can transcribe the audio but lacks other required functionality.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/analyze-content-ai/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 15,
    "question_text": "Your company plans to deploy an AI agent to analyze customer feedback and generate insights from text.\nYou need to select the most suitable Azure AI Service for text analysis.\nWhat should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Language in Foundry Tools.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Azure Speech in Foundry Tools.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure Translator in Foundry Tools.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Azure Vision in Foundry Tools.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Azure Language in Foundry Tools is the most appropriate service for this scenario as it specializes in analyzing and deriving insights from natural language text, directly addressing the requirements for text analysis. Azure Speech in Foundry Tools is not suitable because it focuses on audio transformations such as speech-to-text and text-to-speech, which are unrelated to the task. Azure Translator in Foundry Tools is designed for translating text between languages and does not provide the necessary capabilities for analyzing and generating insights from text. Azure Vision in Foundry Tools is focused on image-related tasks and lacks the functionality required for processing natural language text.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/get-started-openai/5-deploy-models",
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/3-azure-ai-services"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 16,
    "question_text": "Your organization is developing an AI agent to assist with document retrieval and analysis using Microsoft Azure services.\nYou need to enable the AI agent to access external data sources for processing documents.\nWhat should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Configure a custom model in Azure Machine Learning.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Enable default capabilities in Azure Document Intelligence in Foundry Tools.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Use Azure Vision in Foundry Tools Service.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use Azure AI Search with built-in skills.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Azure AI Search is the most suitable solution because it provides the required functionality for accessing and analyzing external documents with built-in skills, which aligns with the scenario's needs. Configuring a custom model in Azure Machine Learning focuses on document analysis but does not address the retrieval aspect, making it unsuitable. Enabling default capabilities in Azure Document Intelligence lacks the specific retrieval functionality needed for the task. Azure Vision in Foundry Tools Service can perform OCR but is not well suited for document processing and lacks the retrieval aspect.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/5-tools-and-sdks"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 17,
    "question_text": "Your organization is using Microsoft Foundry Agent Service to automate customer support workflows.\nYou need to configure an agent to interact with external APIs and use real-time data for responses.\nEach correct answer presents part of the solution. Which three actions should you perform?",
    "options": [
      {
        "letter": "a",
        "text": "Define tools for API access.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Deploy the agent using Azure SDK.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Manually parse API responses in code.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Configure the Bing Search tool.",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "Set up an Azure AI Search resource.",
        "is_correct": false
      },
      {
        "letter": "f",
        "text": "Use Blob Storage for conversation state.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a",
      "b",
      "d"
    ],
    "explanation": "Defining tools for API access ensures the agent can interact with external APIs effectively. Deploying the agent using Azure SDK makes it operational within the Azure environment, enabling API interactions. Configuring the Bing Search tool enables the agent to access real-time data. Setting up an Azure AI Search resource is optional and does not directly contribute to enabling API interaction for the agent. Manually parsing API responses is unnecessary because Microsoft Foundry Agent Service automates tool calling and response handling. Using Blob Storage for conversation state is unnecessary because Microsoft Foundry Agent Service manages conversation state through threads.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/develop-ai-agent-azure/"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 18,
    "question_text": "You develop AI agents for your company by using the Microsoft Foundry Agent Service. The agents use tools to connect to external systems, APIs, and services.\nThe company requires that all agents align with Microsoft Responsible AI principles and operate within approved business and customer-support scenarios.\nYou need to identify intended and supported use cases for the agents.\nWhich three scenarios represent appropriate use cases? Each correct answer presents part of the solution.",
    "options": [
      {
        "letter": "a",
        "text": "A city clerk uses an agent to categorize incoming service requests, assign them to the right department, and compile simple status updates.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "A hospital administrative assistant deploys an agent to collate standard operational procedures, staff directories, and shift policies into concise orientations for new nurses.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "A lawyer deploys an agent to review the legal documents of a client to identify the risks of a new business venture.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "A local boutique owner deploys an agent that recommends gift options based on a customer’s stated needs and past purchases, guiding shoppers responsibly through complex product catalogs.",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "A nurse deploys an agent to diagnose a patient and prescribe medication.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a",
      "b",
      "d"
    ],
    "explanation": "The appropriate use cases are those where AI agents support low-risk, assistive, and productivity-focused scenarios with clear human oversight and alignment to Microsoft Responsible AI principles. Using an agent to categorize and route city service requests, summarize internal operational information for onboarding, or responsibly recommend products to customers are all explainable, auditable tasks that augment human decision-making without replacing regulated professional judgment. In contrast, deploying an agent to perform legal risk analysis or to diagnose patients and prescribe medication involves high-risk, regulated domains that require licensed professionals and accountability, making them inappropriate and noncompliant with Responsible AI requirements for safety, reliability, and fairness.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/intro-ai-agent-service-security-controls/1-understand-azure-ai-agent-service",
      "https://learn.microsoft.com/en-us/training/modules/develop-ai-agents-vs-code/1-introduction",
      "https://learn.microsoft.com/en-us/training/modules/develop-ai-agent-azure/1-introduction",
      "https://learn.microsoft.com/en-us/training/modules/ai-agent-fundamentals/4-azure-ai-agent-service",
      "https://learn.microsoft.com/en-us/training/modules/maximize-cost-efficiency-ai-agent-development/2-custom-built-ai-agents"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 19,
    "question_text": "You have a Microsoft Foundry resource.\nYou need to create an agent by using the Microsoft Foundry Agent Service. The agent will use ground prompts with contextual data and document the sources.\nWhich three elements should you specify in the agent configuration? Each correct answer presents part of the solution.",
    "options": [
      {
        "letter": "a",
        "text": "instructions",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "model deployment",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "name",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "tools",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "YAML file",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a",
      "b",
      "c"
    ],
    "explanation": "When creating an agent by using the Microsoft Foundry Agent Service, you must specify the agent name, the model deployment, and the instructions in the agent configuration because these elements define the agent’s identity, the foundation model it uses, and how it behaves when applying grounded prompts and contextual data, including documenting sources. Tools are optional and only required when the agent must call external capabilities, and a YAML file itself is not a configuration element but merely one possible format used to define the agent. Therefore, specifying instructions, model deployment, and name is required to successfully create the agent using the Agent Service.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/develop-ai-agent-azure/1-introduction",
      "https://learn.microsoft.com/en-us/training/modules/develop-ai-agent-azure/3-how-use-agent-service",
      "https://learn.microsoft.com/en-us/training/modules/ai-agent-fundamentals/4-azure-ai-agent-service",
      "https://learn.microsoft.com/en-us/training/modules/develop-ai-agents-vs-code/3-agent-development"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 20,
    "question_text": "You have an app named App1 that analyzes social media mentions and determines whether comments are positive or negative.\nDuring testing, you notice that App1 generates negative sentiment analysis in response to customer feedback that contains positive feedback.\nYou need to ensure that App1 includes more granular information during the analysis.\nWhat should you add to the API requests?",
    "options": [
      {
        "letter": "a",
        "text": "loggingOptOut=true",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "StringIndexType=TextElements_v8",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "opinionMining=true",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "opinionMining=true will add aspect-based sentiment analysis, which in turn will make the sentiment more granular so that positive and negative in a single sentence can be returned.\nloggingOptOut=true will opt out of logging and StringIndexType=TextElements_v8 will set the returned offset and length values to correspond with TextElements.",
    "references": [
      "https://learn.microsoft.com/rest/api/language/text-analysis-runtime/analyze-text?view=rest-language-2023-04-01&tabs=HTTP",
      "https://learn.microsoft.com/azure/cognitive-services/language-service/sentiment-opinion-mining/how-to/call-api",
      "https://learn.microsoft.com/training/modules/extract-insights-text-with-text-analytics-service/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 21,
    "question_text": "You are building an app that will analyze resumes and remove names and addresses.\nYou need to configure the Azure Language in Foundry Tools Personally Identifiable Information (PII) detection feature for the app.\nWhich categories should you specify in the request?",
    "options": [
      {
        "letter": "a",
        "text": "Person, Address, and IP",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Person and Address only",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Person, PersonType, and Address",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "Person and Address will detect names and addresses.\nPersonType will also remove job roles. IP will also remove IP addresses.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/personally-identifiable-information/concepts/entity-categories",
      "https://learn.microsoft.com/training/modules/publish-use-language-understand-app/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 22,
    "question_text": "You are building an app that will analyze the sentiment of user feedback by using Azure Language in Foundry Tools.\nYou have a test document named Test.docx that contains one positive sentence and multiple neutral sentences.\nYou need to validate the app by analyzing Test.docx.\nWhich label will the app return for Test.docx?",
    "options": [
      {
        "letter": "a",
        "text": "mixed",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "negative",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "neutral",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "positive",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "If there is at least one positive sentence in the document, and the rest of the document is neutral, then the document label is positive.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/sentiment-opinion-mining/how-to/call-api",
      "https://learn.microsoft.com/training/modules/extract-insights-text-with-text-analytics-service/"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 23,
    "question_text": "You are building an app that will enable users to create notes by using speech.\nYou need to recommend the Azure Speech in Foundry Tools service model to use. The solution must support noisy environments.\nWhich model should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "base",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "base with customizations",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "custom speech-to-text",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "default",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "The custom speech-to-text model is correct, as you need to adapt the model because a factory floor might have ambient noise, which the model should be trained on.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/speech-service/faq-stt",
      "https://learn.microsoft.com/training/modules/transcribe-speech-input-text/"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 24,
    "question_text": "You are building an app that will recognize the intent and entities of user utterances in real time using the Azure Speech in Foundry Tools Service.\nYou need to implement the pattern matching intent recognition mechanism. The solution must only detect entities that you define in a catalog of phrases.\nWhich entity type should you use?\nSelect only one answer.",
    "options": [
      {
        "letter": "a",
        "text": "the List entity using Fuzzy mode",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "the List entity using Strict mode",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "the prebuilt entity using Fuzzy mode",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "the prebuilt entity using Strict mode",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "the regex entity using Fuzzy mode",
        "is_correct": false
      },
      {
        "letter": "f",
        "text": "the regex entity using Strict mode",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "The List entity is made up of a list of phrases that will guide the engine on how to match the text. When an entity has an ID of type List and is in Strict mode, the engine will only match if the text in the slot appears in the list.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/speech-service/pattern-matching-overview#types-of-entities",
      "https://learn.microsoft.com/training/modules/transcribe-speech-input-text/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 25,
    "question_text": "You are building an app that will recognize the intent and entities of user utterances in real-time.\nYou are evaluating the use of intent recognition with the Azure Speech in Foundry Tools and Azure Language in Foundry Tools services or simple pattern matching.\nWhen should you use pattern matching?",
    "options": [
      {
        "letter": "a",
        "text": "You are only interested in matching strictly what the user said.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "You must manage the model by using a web app.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "You must use a machine learned entity.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "You must use a prebuilt entity.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "The only option is to use pattern matching over Language Understanding when you want to strictly match what a user said, as the incorrect options are only available in Language Understanding.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/speech-service/how-to-use-custom-entity-pattern-matching?tabs=jre%2Cwindows%2Cubuntu%2Cmaven&pivots=programming-language-csharp",
      "https://learn.microsoft.com/training/modules/transcribe-speech-input-text/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 26,
    "question_text": "You are building a model that uses Conversational Language Understanding (CLU).\nYou need to train the model.\nWhich training methods can you use?",
    "options": [
      {
        "letter": "a",
        "text": "advanced, deterministic, and standard",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "advanced only",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "deterministic only",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "standard and advanced only",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "standard only",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Both standard and advanced are from CLU. Deterministic is a method from Language Understanding.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/conversational-language-understanding/how-to/train-model?tabs=language-studio#training-modes",
      "https://learn.microsoft.com/azure/cognitive-services/luis/how-to/train-test#change-deterministic-training-settings-using-the-version-settings-api",
      "https://learn.microsoft.com/training/modules/build-language-understanding-model/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 27,
    "question_text": "You plan to build a chatbot that will help users answer FAQs.\nYou need to identify which scenarios are suitable for use with the Azure Language in Foundry Tools question answering service.\nWhich three scenarios should you identify? Each correct answer presents a complete solution.",
    "options": [
      {
        "letter": "a",
        "text": "when you have a bot conversation that includes dynamic information",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "when you have a bot conversation that includes static information",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "when you have dynamic information in a knowledge base of answers",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "when you have static information in a knowledge base of answers",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "when you need to provide the same answer to a request, question, or command",
        "is_correct": true
      },
      {
        "letter": "f",
        "text": "when you need to provide unique answers to each request, question, or command",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b",
      "d",
      "e"
    ],
    "explanation": "Question answering only works with static information, not with dynamic information. In addition, it will always provide the same answer to the same question.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/question-answering/overview",
      "https://learn.microsoft.com/training/modules/build-qna-solution-qna-maker/"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 28,
    "question_text": "You are creating an assistant based on a generative Microsoft Foundry model.\nYou plan to use the system message component for prompts in the solution.\nWhich two capabilities does the system message offer for the model? Each correct answer presents part of the solution.",
    "options": [
      {
        "letter": "a",
        "text": "defines the data sources that should not be included in the model",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "defines what the model should and should not do",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "detects which language is being used in a prompt",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "helps define the assistant’s personality",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "b",
      "d"
    ],
    "explanation": "The system message is included at the beginning of the prompt and is used to prime the model with context, instructions, or other information relevant to the use case. You can use the system message to describe the assistant’s personality, define what the model should and should not answer, and define the format of model responses.",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 29,
    "question_text": "Your company uses an Azure OpenAI in Foundry Models service to generate images based on text prompts.\nYou need to configure the API request to ensure optimized image quality and resolution.\nEach correct answer presents part of the solution. Which two actions should you take?",
    "options": [
      {
        "letter": "a",
        "text": "Set 'n' to 10.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Set 'output_format' to JPEG.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Set 'quality' to hd.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Specify 'size' as 1024x1792.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "c",
      "d"
    ],
    "explanation": "To optimize image quality and resolution, 'quality' should be set to hd to ensure finer details and better consistency, and 'size' should be specified as 1024x1792 to meet the required resolution. Setting 'n' only determines the number of images generated and does not impact quality or resolution. Configuring 'output_format' affects the image format but does not influence image quality or resolution.",
    "references": [
      "https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/dall-e?tabs=dalle-3"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 30,
    "question_text": "Your company is developing a chatbot using an Azure OpenAI in Foundry Models service to provide answers based on the company's internal knowledge base.\nYou need to improve the chatbot's ability to retrieve and process information from the knowledge base.\nEach correct answer presents part of the solution. Which two actions should you take?",
    "options": [
      {
        "letter": "a",
        "text": "Index the knowledge base with AI Search.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Integrate the Embeddings API.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Train a custom language model.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use AI Vision for document analysis.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a",
      "b"
    ],
    "explanation": "Integrating the Embeddings API enhances semantic processing of queries, enabling the chatbot to understand user intent and respond accurately. Indexing the knowledge base with AI Search creates a searchable index, allowing the chatbot to efficiently retrieve relevant information and provide accurate responses. Training a custom language model is unnecessary because pre-trained models like GPT-4 are already optimized for this task and can handle the required functionality without additional training. Using AI Vision for document analysis is not applicable in this scenario because it is designed for image analysis rather than processing textual knowledge base documents.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/3-azure-ai-services",
      "https://learn.microsoft.com/en-us/training/modules/prepare-to-develop-ai-solutions-azure/4-understand-considerations-for-ai-engineers"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 31,
    "question_text": "You are building a web app that will generate images based on user prompts. The app will use the DALL-E 3 Azure OpenAI model.\nYou need to ensure that HTTP requests against the Azure OpenAI API successfully generate images.\nWhich three HTTP header properties should you include? Each correct answer presents part of the solution.",
    "options": [
      {
        "letter": "a",
        "text": "the API version used in this operation",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "the name of the Azure OpenAI service resource",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "the name of the DALL-E 3 model deployment",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "the quality of the generated images",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "the style of the generated images",
        "is_correct": false
      },
      {
        "letter": "f",
        "text": "the user’s prompt",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a",
      "b",
      "c"
    ],
    "explanation": "The name of the Azure OpenAI resource, the name of the DALL-E 3 model deployment, and the API version to be used are the three required header properties for HTTP requests. The other answers are valid for use in the HTTP body but not the header.",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/openai/reference"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 32,
    "question_text": "You are creating an application that will use Azure OpenAI REST API services. The application uses a REST call to a DALL-E model to generate images. The three parameters in the REST call are prompt, n, and size.\nWhat does the size parameter indicate?",
    "options": [
      {
        "letter": "a",
        "text": "the number of responses that you want returned",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "the size of the images in bytes",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "the size of the images in kilobytes",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "the size of the images in pixel resolution",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "To make a REST call to the services, you need the endpoint and authorization key for the Azure OpenAI service resource you provisioned in Azure. You initiate the image generation process by submitting a POST request to the service endpoint that has the authorization key in the header. The request must contain the following parameters in a JSON body:\nprompt: The description of the image to be generated\nn: The number of images to be generated\nsize: The resolution of the image to be generated (256x256, 512x512, or 1024x1024)",
    "references": [
      "https://learn.microsoft.com/training/modules/generate-images-azure-openai/4-dall-e-rest-api"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 33,
    "question_text": "Your organization plans to implement a generative AI solution to assist the customer success by summarizing customer complaints and questions.\nYou need to ensure the model provides accurate summaries while reducing costs.\nWhat action should you take?",
    "options": [
      {
        "letter": "a",
        "text": "Apply prompt engineering without external data sources.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Deploy a model without grounding data and rely on pre-trained capabilities.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Fine-tune a model using customer query data without an orchestration layer.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use Retrieval-Augmented Generation (RAG) with Azure AI Search for grounding data.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Using Retrieval-Augmented Generation (RAG) with Azure AI Search ensures the model provides accurate and contextually relevant summaries by incorporating domain-specific data. Deploying a model without grounding data relies solely on pre-trained capabilities, resulting in less accurate outputs. Fine-tuning a model without an orchestration layer fails to dynamically integrate real-time contextual data, limiting its effectiveness, and is significantly more expensive. Applying prompt engineering without external data sources improves response quality but does not address the need for domain-specific grounding.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/build-copilot-ai-studio/2-ground-language-model"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 34,
    "question_text": "You use a text-generation model deployed in Microsoft Foundry.\nThe model produces responses that vary in tone and creativity.\nYou need to reduce the randomness of the model’s output to make its responses more predictable and consistent.\nWhich parameter should you configure?",
    "options": [
      {
        "letter": "a",
        "text": "max_tokens",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "role",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "stop",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "temperature",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "You should configure the temperature parameter because it directly controls the randomness and creativity of a text-generation model’s output in Microsoft Foundry. Lowering the temperature value makes responses more focused, deterministic, and consistent, while higher values increase variability and creativity. The max_tokens parameter only limits the length of the response, the role parameter defines the message author in a chat interaction, and the stop parameter specifies sequences that halt generation rather than influencing randomness or tone.",
    "references": [
      "https://learn.microsoft.com/en-us/azure/ai-foundry/openai/prompt-completion"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 35,
    "question_text": "You have a generative AI application that was deployed by using Microsoft Foundry. The application uses a flow created in Microsoft Foundry.\nYou plan to collect trace data for each request, aggregate metrics, and user feedback for the application.\nYou need to enable tracing and collect feedback for the deployment by using Microsoft Foundry. The solution must minimize administrative effort.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Add environment variables to the YAML file of the deployment.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Add properties to the YAML file of the deployment.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "From Deployment, configure the Application Insights diagnostics settings.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Modify the YAML file of the flow deployment",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "You should configure the Application Insights diagnostics settings from the deployment in Microsoft Foundry because this option natively enables request tracing, aggregated metrics, and user feedback collection with minimal administrative effort. Microsoft Foundry integrates directly with Application Insights at the deployment level, allowing telemetry and feedback to be enabled through the portal without modifying YAML files or flow definitions. Adding environment variables or properties to deployment YAML files does not activate tracing or feedback features, and modifying the flow YAML only affects orchestration logic rather than deployment-level monitoring and diagnostics.",
    "references": [
      "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/trace-production-sdk?view=foundry-classic",
      "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/trace-application?view=foundry-classic",
      "https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-enable-trace-feedback-for-deployment?view=azureml-api-2"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 36,
    "question_text": "You are building an app that will extract specific information from scanned receipts.\nWhich service should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Application Insights",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Azure Language in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure Document Intelligence in Foundry Tools",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Azure AI Metrics Advisor",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Azure Document Intelligence in Foundry Tools is capable of automatically extracting information from given documents by using machine learning.\nAzure AI Metrics Advisor, Azure Application Insights, and Azure Language in Foundry Tools do not allow you to extract specific data from scanned receipts.",
    "references": [
      "https://learn.microsoft.com/training/modules/use-prebuilt-form-recognizer-models/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 37,
    "question_text": "You are building an app that will extract insights from video files.\nYou need to identify which service to use. The solution must ensure that you can customize the language model used.\nWhat should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Language in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Azure Communication Services",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure Vision in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Azure AI Video Indexer",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "The only service that can customize the language model for a solution based on gaining insights from videos in Azure AI Video Indexer.",
    "references": [
      "https://learn.microsoft.com/azure/azure-video-indexer/customize-language-model-overview",
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 38,
    "question_text": "You are building an Azure Language in Foundry Tools solution.\nYou need to deploy the solution to a location without internet connectivity.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Deploy the solution to a Docker host container.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Download the model and deploy it to a virtual machine.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Use a Microsoft Foundry Service standard instance.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "You can use disconnected containers to host Language Understanding on-premises.\nThere is no virtual machine set up to run a Language Understanding instance and a standard instance still has Language Understanding running in the cloud.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/containers/disconnected-containers",
      "https://learn.microsoft.com/training/modules/investigate-container-for-use-cognitive-services/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 39,
    "question_text": "You have a Microsoft Foundry service named Foundry1.\nYou need to deploy a foundation model to Foundry1 that can generate content, summarize text, understand images, perform semantic search, and generate code.\nWhich model provider should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Cohere",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Meta",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Mistral",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "OpenAI",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "OpenAI is the appropriate model provider because Azure OpenAI models available in Microsoft Foundry (such as GPT-4–class models) natively support a broad set of capabilities required by the scenario, including content generation, text summarization, image understanding, semantic search through embeddings, and code generation, all of which can be deployed and managed directly within a Foundry service. Cohere models primarily focus on retrieval-augmented generation and summarization, Meta models emphasize large-scale text generation without integrated multimodal and code features in Foundry, and Mistral models provide strong text reasoning but lack the full multimodal and code-generation support needed to meet all the stated requirements.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/optimize-spend-and-performance-with-azure-ai-foundry-provisioned-reservations/azure-ai-foundry-overview",
      "https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/deployments-overview"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 40,
    "question_text": "You plan to build an app that will use Microsoft Foundry Service.\nYou need to identify the methods that can be used to authenticate to Azure AI Services.\nWhich two methods can you use? Each correct answer presents a complete solution.",
    "options": [
      {
        "letter": "a",
        "text": "a SAML token",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "a subscription key",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Microsoft Entra ID",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Kerberos",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b",
      "c"
    ],
    "explanation": "You can use a single or multi-service subscription keys to authenticate to Azure AI Services. You can also authenticate to Azure AI Services by using a Microsoft Entra ID service principal and role-based access control (RBAC).\nAzure AI Services do not support authentication by using SAML tokens or Kerberos.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/authentication?tabs=powershell#authenticate-with-an-access-token",
      "https://learn.microsoft.com/training/modules/secure-cognitive-services/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 41,
    "question_text": "You are building an app that will use Azure AI Custom Vision. The app will be deployed to a virtual machine in Azure.\nYou enable firewall rules for your Azure AI Services account.\nYou need to ensure that the app can access the service through a service endpoint.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Assign a role-based access control (RBAC) role to the Azure AI Custom Vision resource.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Grant access to a specific virtual network.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Grant access to an internet IP range.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Include an access token in the Authorization header.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "If you enable the firewall for the Azure AI Services account, you need to allow network access to the service. You can achieve this by either allowing access from a specific virtual network or adding an IP range to the firewall rules. In this situation, the app is deployed to a virtual machine in Azure, which resides in a virtual network. You can provide access to virtual networks in Azure to access specific service endpoints.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/cognitive-services-virtual-networks?context=%2Fazure%2Fcognitive-services%2Fcustom-vision-service%2Fcontext%2Fcontext&tabs=portal",
      "https://learn.microsoft.com/training/modules/secure-cognitive-services/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 42,
    "question_text": "You have an Azure App Services web app named App1.\nYou need to configure App1 to use Microsoft Foundry to authenticate by using Microsoft Entra ID. The solution must meet the following requirements:\nMinimize administrative effort.\nUse the principle of least privilege.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Create a secret and store the secret in an Azure key vault. Assign App1 role-based access control (RBAC) permissions to the secret.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Create a Microsoft Entra app registration and enable certificate-based authentication.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "From App1, enable a managed identity and assign role-based access control (RBAC) permissions to Microsoft Foundry.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "From PowerShell, create a secret that never expires.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "With a managed identity, the rotation of the secrets (certificates) is done automatically.\nYou still need to rotate secrets by using the key vault, and you cannot create secrets that never expire from the portal. It is not considered best practice to create one with PS1 or the CLI, and a certificate will also expire at some point.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/authentication?tabs=powershell#authorize-access-to-managed-identities",
      "https://learn.microsoft.com/training/modules/secure-cognitive-services/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 43,
    "question_text": "You are building an app that will use Azure AI Custom Vision. The app will be deployed to an internet host.\nYou enable firewall rules for your Azure AI Services account.\nYou need to ensure that the app can access the Azure AI Custom Vision service over the internet.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Assign a role-based access control (RBAC) role to the Azure AI Custom Vision service.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Grant access to a specific virtual network.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Grant access to an internet IP range.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Include an access token in the Authorization header.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "If you enable the firewall for the Azure AI Services account, you need to allow network access to the service. You can achieve this by either allowing access from a specific virtual network or adding an IP range to the firewall rules. In this situation, the app is deployed to the internet, and you can provide specific internet hosts access by adding an IP or range of IPs to the firewall rules.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/cognitive-services-virtual-networks?context=%2Fazure%2Fcognitive-services%2Flanguage-service%2Fcontext%2Fcontext&tabs=portal",
      "https://learn.microsoft.com/training/modules/secure-cognitive-services/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 44,
    "question_text": "You are provisioning an Azure OpenAI in Foundry Models service resource.\nYou need to ensure that the resource is only available to applications that are hosted in your Azure subscription.\nWhich network security setting should you configure?",
    "options": [
      {
        "letter": "a",
        "text": "All networks",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "All networks, and a network security group to control traffic",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Disabled, and allow a private endpoint connection to establish access",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Selected networks",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Because the requirements state that access to the resource should only be for applications hosted in the Azure subscription, setting the network option to Disabled and configuring a private endpoint meets this requirement.\nAll networks permit access to any network, including the internet. Selected networks access to networks outside of Azure, if configured that way.",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 45,
    "question_text": "You have a website that enables users to upload and share PNG image files.\nYou implement a Microsoft Foundry Content Safety solution for the website.\nThe users report that some images fail to display after they are uploaded.\nYou need to ensure that the solution assigns proper content flags to the uploaded images.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Adjust the content categories to be used.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Adjust the severity level.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Convert the images to a different file format.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Remove the severity level.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "Best practices for improving system performance:\nMonitor the system's performance regularly to ensure that the tradeoff is appropriate for your use case.\nModify the severity levels for blocking based on user feedback and observed trends in content safety.",
    "references": [
      "https://learn.microsoft.com/legal/cognitive-services/content-safety/transparency-note?context=%2Fazure%2Fai-services%2Fcontent-safety%2Fcontext%2Fcontext"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 46,
    "question_text": "Your organization is developing an AI-powered e-learning platform that generates personalized educational content based on user inputs and reference documents. Concerns exist about generating inappropriate or misleading educational materials.\nYou need to ensure AI-generated educational content complies with academic standards and avoids inappropriate material.\nWhat should you implement to address these concerns?",
    "options": [
      {
        "letter": "a",
        "text": "Enable Azure Translator.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Implement Microsoft Foundry Content Safety.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Train a custom model with additional datasets.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use Azure Language in Foundry Tools.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "Azure Content Safety is the most suitable solution because it analyzes user inputs and reference documents to detect and block harmful or policy-violating content, ensuring generated educational materials are safe and compliant. Azure AI Services Text Analytics evaluates sentiment but does not address content safety or compliance. Training a custom model improves content relevance but does not safeguard against inappropriate or misleading outputs. Azure Translator enhances accessibility through multilingual support but does not address the core issue of ensuring content safety and compliance.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/responsible-content-safety/2-what-is-content-safety"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 47,
    "question_text": "You are building an app that will detect the color scheme of uploaded images.\nYou are evaluating using the Image Analysis API to detect the dominant background color of an image.\nWhich color can the API return as a dominant background color?",
    "options": [
      {
        "letter": "a",
        "text": "azure",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "maroon",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "silver",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "teal",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Only a certain set of colors can be returned by the API. The set of possible returned colors is black, blue, brown, gray, green, orange, pink, purple, red, teal, white, and yellow.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/computer-vision/concept-detecting-color-schemes",
      "https://learn.microsoft.com/training/modules/analyze-images/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 48,
    "question_text": "You are building an app that will use Azure Vision in Foundry Tools to extract text from scanned images of handwritten text.\nWhich Azure Vision in Foundry Tools feature should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Azure AI Custom Vision",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "handwriting analysis",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Image Analysis",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "optical character recognition (OCR)",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "Spatial Analysis",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "OCR is the only visual feature that can extract text from images.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview",
      "https://learn.microsoft.com/training/modules/read-text-images-documents-with-computer-vision-service/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 49,
    "question_text": "You need to build an app that will use Azure Vision in Foundry Tools to analyze and detect animals in images.\nWhich type of project should you use?",
    "options": [
      {
        "letter": "a",
        "text": "image classification",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "image detection",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "object analysis",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "object classification",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "object detection",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "e"
    ],
    "explanation": "Object detection returns the coordinates in an image where the applied label(s) can be found, while image classification applies one or more labels to an entire image.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/custom-vision-service/overview",
      "https://learn.microsoft.com/training/modules/detect-objects-images/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 50,
    "question_text": "Your organization has trained a custom vision model using Microsoft Azure Custom Vision to classify product images. The model has satisfactory performance metrics.\nYou need to make the model available for predictions in a production environment.\nWhat should you do next?",
    "options": [
      {
        "letter": "a",
        "text": "Create a new Azure resource for the model.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Export the model for local use.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Publish the model and retrieve its Prediction URL and key.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Re-train the model with additional data.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Publishing the trained iteration and retrieving its Prediction URL and key is the appropriate action to make the model accessible for programmatic predictions in a production environment. Exporting the model for local use is not aligned with the goal of leveraging the Microsoft Azure Custom Vision Prediction API. Re-training the model is unnecessary as the current performance metrics are satisfactory and does not contribute to the deployment process. Creating a new Azure resource is unrelated to the task of deploying the trained model and does not address the stated goal.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/3-get-started",
      "https://learn.microsoft.com/en-us/training/modules/generate-images-azure-openai/4-dall-e-rest-api"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 51,
    "question_text": "You are developing a custom vision model using Microsoft Azure AI Custom Vision to detect specific objects in images. The dataset includes labeled images for training and validation.\nYou need to ensure that the trained model meets the required accuracy by assessing its performance.\nWhat action should you take?",
    "options": [
      {
        "letter": "a",
        "text": "Deploy the model without testing.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Evaluate precision and recall metrics.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Source new images for testing.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Retrain the model with a new dataset.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "Evaluating precision and recall metrics is the most effective way to assess the performance of an object detection model, as these metrics provide a detailed understanding of its accuracy and ability to identify objects correctly. Deploying the model without testing bypasses the critical step of ensuring its reliability and effectiveness. Sourcing new images for testing isn't necessary with a dataset already containing validation images. Retraining the model with a new dataset focuses on improving data quality rather than assessing the current model's performance, making it unsuitable for the given objective.",
    "references": [
      "https://learn.microsoft.com/en-us/azure/ai-services/custom-vision-service/test-your-model"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 52,
    "question_text": "You are developing a custom vision model using Microsoft Azure Custom Vision to classify images of various food items. The dataset includes images labeled as 'vegetable-fruit', 'dessert', and 'soup'.\nYou need to optimize the training process to improve the model's performance metrics.\nEach correct answer presents part of the solution. Which two actions should you perform?",
    "options": [
      {
        "letter": "a",
        "text": "Select the 'Food' domain.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Select the 'General' domain.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Set a high probability threshold during training.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use consistent tags to label images.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "a",
      "d"
    ],
    "explanation": "Using consistent tags to label images ensures that the model can accurately associate visual features with their respective categories, which is essential for effective training. Selecting the 'Food' domain optimizes the model for food-related image classification tasks, leveraging domain-specific features to enhance performance. Setting a high probability threshold during training is not suitable because it could reduce recall by limiting the number of classifications detected. Similarly, selecting the 'General' domain is not ideal as it lacks the specialization required for food-related tasks, potentially leading to suboptimal results.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/custom-text-classification/2-understand-types-of-classification-projects",
      "https://learn.microsoft.com/en-us/azure/ai-services/custom-vision-service/select-domain"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 53,
    "question_text": "You are building an app that will use Azure Vision in Foundry Tools to detect the presence of people in a video feed.\nWhich Azure Vision in Foundry Tools feature should you use?",
    "options": [
      {
        "letter": "a",
        "text": "face detection",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Image Analysis",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "optical character recognition (OCR)",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Spatial Analysis",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "The only visual feature that provides this capability is Spatial Analysis, as OCR, Image Analysis, and face detection are not meant to analyze the presence of people in a video feed.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview",
      "https://learn.microsoft.com/training/modules/analyze-video/"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 54,
    "question_text": "You are building a video processing app that will use Azure AI Video Indexer.\nYou need to configure the training and learning phases for the app. The solution must train the model based on the probability of specific word combinations by using a custom Language model.\nWhich three practices should be followed for the training data? Each correct answer presents a complete solution.",
    "options": [
      {
        "letter": "a",
        "text": "Include at least 500,000 sentences.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Include multiple examples of spoken sentences.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Include special characters such as ~, #, @, %, and &.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Provide multiple adaptation options.",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "Put only one sentence per line.",
        "is_correct": true
      },
      {
        "letter": "f",
        "text": "Repeat the identical sentence multiple times.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b",
      "d",
      "e"
    ],
    "explanation": "When training the model, you should avoid repeating an identical sentence multiple times, as it may create bias against the rest of the input.\nYou should avoid including uncommon symbols (~, # @ % &), as they will be discarded. The sentences in which they appear will also be discarded.\nYou should also avoid putting inputs that are too large, such as hundreds of thousands of sentences, because doing so will dilute the effect of boosting.",
    "references": [
      "https://learn.microsoft.com/azure/azure-video-indexer/customize-language-model-overview",
      "https://learn.microsoft.com/training/modules/analyze-video/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 55,
    "question_text": "You are building a solution that uses Azure AI Search.\nYou need to save normalized binary files as projections.\nWhich type of projection should you use?",
    "options": [
      {
        "letter": "a",
        "text": "files",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "objects",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "tables",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Tables are used for data that is best represented as rows and columns, or whenever you need granular representations of your data.\nFiles are used when you need to save normalized, binary image files. Objects are used when you need the full JSON representation of your data and enrichments in one JSON document.",
    "references": [
      "https://learn.microsoft.com/azure/search/knowledge-store-projection-overview#types-of-projections-and-usage",
      "https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 56,
    "question_text": "You are building an app will use Azure AI Search.\nYou need to index a collection of documents.\nWhat is the first stage of the indexing process?",
    "options": [
      {
        "letter": "a",
        "text": "document cracking",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "field mapping",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "output field mapping",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "push into index",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "skillset execution",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Document cracking is the process of opening files and extracting content. It is the first stage of the indexing process.\nText-based content can be extracted from files in a service, rows in a table, or items in a container or collection. If you add a skillset and image skills, document cracking can also extract images and queue them for image processing.",
    "references": [
      "https://learn.microsoft.com/azure/search/search-indexer-overview",
      "https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 57,
    "question_text": "Your organization processes a large volume of scanned vendor documents, some of which are poorly scanned or contain creases.\nYou need to extract structured data such as vendor details, totals, and line items from these invoices.\nWhat should you use to achieve this goal?",
    "options": [
      {
        "letter": "a",
        "text": "Train a custom model in Azure Document Intelligence in Foundry Tools Studio.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Use Azure Vision in Foundry Tools OCR.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Use the Invoice model in Azure Document Intelligence in Foundry Tools.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Use the Business Card model in Azure Document Intelligence in Foundry Tools.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "The Invoice model in Azure Document Intelligence in Foundry Tools is the optimal choice for processing invoices due to its specialized capabilities for extracting structured data, even from poorly scanned documents. Training a custom model is unnecessary because the Invoice model already meets the requirements. Azure Vision in Foundry Tools OCR lacks the ability to extract structured data, making it unsuitable for this task. The Business Card model is designed for business cards and does not provide the necessary functionality for processing invoices.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/use-prebuilt-form-recognizer-models/4-use-financial-id-tax-models"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 58,
    "question_text": "You are building an app that will extract data from legal documents.\nYou plan to use the Azure Document Intelligence in Foundry Tools prebuilt-read model.\nWhich three document formats does the prebuilt-read model support? Each correct answer presents a complete solution.",
    "options": [
      {
        "letter": "a",
        "text": "JSON",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Microsoft Excel",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Microsoft Word",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "PDF",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "TXT",
        "is_correct": false
      },
      {
        "letter": "f",
        "text": "XML",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b",
      "c",
      "d"
    ],
    "explanation": "The prebuilt-read model supports PDF, Excel, and Word files.\nTXT, JSON, and XML are unsupported by the prebuilt-read model of Azure Document Intelligence in Foundry Tools",
    "references": [
      "https://learn.microsoft.com/azure/applied-ai-services/form-recognizer/concept-read?view=form-recog-3.0.0",
      "https://learn.microsoft.com/training/modules/work-form-recognizer/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 59,
    "question_text": "You have an Azure Document Intelligence in Foundry Tools resource.\nYou need to use the resource in the Document Intelligence Studio to extract key-value pairs, selection marks, and tables from documents.\nWhat should you use?",
    "options": [
      {
        "letter": "a",
        "text": "a custom model",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "the general document model",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "the layout model",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "the receipt model",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "You should use the general document model because it is designed to extract key-value pairs, selection marks, tables, and other common entities from a wide variety of document types in Azure Document Intelligence Studio without requiring custom training. Custom models are intended for highly specialized or organization-specific document formats and require training, the layout model focuses on extracting text, tables, and structural layout information rather than rich key-value semantics, and the receipt model is limited to receipt-specific fields and scenarios, making it unsuitable for general-purpose document extraction.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/9-form-recognizer-studio",
      "https://learn.microsoft.com/en-us/training/modules/ai-information-extraction/5-document-intelligence"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 60,
    "question_text": "You build an app that enables users to upload scans of invoices.\nYou need to extract text and key/value pairs from the scanned invoices.\nWhich service should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Azure AI Search",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Azure Document Intelligence in Foundry Tools",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Azure AI Immersive Reader",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Azure AI Metrics Advisor",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "The Azure Document Intelligence in Foundry Tools is a service that lets you build automated data processing software by using machine learning technology. It allows you to identify and extract text and key/value pairs.\nAzure AI Metrics Advisor uses AI to perform data monitoring and anomaly detection in time series data. Azure AI Immersive Reader is a tool that implements techniques to improve reading comprehension for new readers, language learners, and people with learning differences. Azure AI Search is a cloud search service to help you identify and explore relevant content at scale.",
    "references": [
      "https://learn.microsoft.com/azure/applied-ai-services/what-are-applied-ai-services",
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 61,
    "question_text": "You are building a GPT-based chat application that will answer questions about your company.\nYou plan to use the Using your data feature in Azure OpenAI to ground the model with your company data.\nWhile testing, you discover that some responses are not accurate enough.\nYou need to configure the Azure OpenAI resource to filter out less-relevant documents for responses.\nWhich parameter should you configure?",
    "options": [
      {
        "letter": "a",
        "text": "Content data",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "File name",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Retrieved documents",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Strictness",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "The Strictness parameter sets the threshold to categorize documents as relevant to your queries. Raising the Strictness parameter value means a higher threshold for relevance and filters out more less-relevant documents for responses. Retrieved documents specifies the number of top-scoring documents from your data index used to generate responses. Content data specifies the fields in your index that contain the main text content of each document. File name specifies the field in your index that contains the original file name of each document.",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data?tabs=ai-search"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 62,
    "question_text": "You are building a GPT-based chat application that will answer questions about your company.\nYou plan to test the application by using strategies defined by Microsoft best practices.\nWhich three prompt engineering strategies should you consider while testing the application? Each correct answer presents a complete solution.",
    "options": [
      {
        "letter": "a",
        "text": "Be Descriptive",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Be minimalistic",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Be simple",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Be Specific",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "Order Matters",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "a",
      "d",
      "e"
    ],
    "explanation": "Be Specific means to leave as little to interpretation as possible. Be Descriptive means to use analogies. Order Matters means that the order in which you present information to the model can affect the output. Therefore, those three are valid best practices. Be simple and be minimalistic do not produce the best results and are, therefore, not best practices",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 63,
    "question_text": "Your organization is developing a customer-facing application that uses Azure OpenAI in Foundry Models to generate personalized responses. The application is connected to a Microsoft Azure AI Search index.\nYou need to configure the application to retrieve relevant data from the search index.\nWhat should you do?\nSelect only one answer.",
    "options": [
      {
        "letter": "a",
        "text": "Deploy an additional Azure OpenAI model.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Enable semantic search.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Increase chunk size for data ingestion.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use keyword search.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "Enabling semantic search enhances the precision and relevance of search results by interpreting the meaning behind query terms, making it the most suitable solution for improving response accuracy in this scenario. Increasing chunk size impacts data processing but does not directly affect the accuracy of retrieved data. Deploying an additional Azure OpenAI model does not address the specific need for improving data retrieval accuracy from the search index. Using keyword search lacks semantic capabilities, which are essential for achieving the desired level of accuracy.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/build-copilot-ai-studio/3-search-data"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 64,
    "question_text": "You are creating an application that references the Azure OpenAI REST API for a DALL-E model.\nYou plan to use thumbnails of the images that DALL-E generates and display them in a table on a webpage.\nYou need to find the image URLs in the JSON response.\nWhich element should you review?",
    "options": [
      {
        "letter": "a",
        "text": "the ids array element",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "the images array element",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "the imageURL array element",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "the result element",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "The result from the initial request does not immediately return the results of the image generation process. Instead, the response includes an operation-location header with a URL for a callback service that your application code can poll until the results of the image generation are ready. The result element includes a collection of url elements, each of which references a PNG image file generated from the prompt.",
    "references": [
      "https://learn.microsoft.com/training/modules/generate-images-azure-openai/4-dall-e-rest-api"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 65,
    "question_text": "You are building a web app that will generate images based on user prompts. The app will use the DALL-E 3 Azure OpenAI model.\nYou need to ensure that HTTP requests against the Azure OpenAI API successfully generate images.\nWhich HTTP body property should you include?",
    "options": [
      {
        "letter": "a",
        "text": "the API version",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "the Azure OpenAI resource name",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "the deployment ID",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "the prompt",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "The prompt is the only required property to be used in the body of the HTTP request when requesting to generate a new image by using the DALL-E 3 Azure OpenAI API. The other properties are used in the HTTP header.",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/openai/reference#image-generation"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 66,
    "question_text": "Your company uses an Azure OpenAI Foundry Models service to generate code snippets for software development.\nYou need to ensure the generated code adheres to your organization's coding standards.\nWhat action should you take to achieve this?",
    "options": [
      {
        "letter": "a",
        "text": "Enable multi-region deployment.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Increase compute resources.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Provide standard-compliant examples in prompts.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Switch to a chat-optimized model.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Providing examples of code adhering to standards in the input prompts ensures the AI model generates code that aligns with those standards, addressing the issue effectively. Enabling multi-region deployment enhances availability and resilience but does not impact code adherence to standards. Increasing compute resources may improve processing speed but does not affect code adherence to standards. Models optimized for chat-based interactions are not specifically designed to improve code adherence to standards.",
    "references": [
      "https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering?tabs=chat"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 67,
    "question_text": "You are deploying a generative AI solution using an Azure OpenAI in Foundry Models service to process customer feedback and generate summaries.\nYou need to optimize the solution for performance and ensure its responses align with the organization's tone and style.\nEach correct answer presents part of the solution. Which three actions should you perform?",
    "options": [
      {
        "letter": "a",
        "text": "Deploy the model on edge devices.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Enable support for multiple languages.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Fine-tune the model with customer feedback data.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Set up monitoring in Azure to track response accuracy and resource usage.",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "Use prompt engineering to refine the model's output.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "c",
      "d",
      "e"
    ],
    "explanation": "To optimize the generative AI solution for performance and ensure its responses align with the organization's tone and style, it is essential to set up monitoring in Azure to continuously evaluate the model's performance and resource usage. Fine-tuning the model with customer feedback data customizes the solution to meet specific organizational requirements, ensuring alignment with tone and style. Additionally, using prompt engineering refines the model's responses by structuring input effectively, enhancing the quality and relevance of the generated content. Deploying the model on edge devices and enabling support for multiple languages do not directly address the stated goals, as they focus on deployment strategy and additional features rather than performance optimization or alignment with tone and style.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/finetune-model-copilot-ai-studio/"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 68,
    "question_text": "You have an Azure Vision in Foundry Tools resource.\nYou plan to use the Azure Vision in Foundry Tools service to identify the location of several objects in an image.\nYou need the service to return the coordinates of the detected objects.\nWhich feature should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Detect brands",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "image tagging",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "object detection",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "recognize domain-specific content",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "You should use object detection because this feature in Azure Vision (Foundry Tools) identifies individual objects within an image and returns their bounding box coordinates, enabling you to determine the precise location of each detected object. Image tagging only returns descriptive labels with confidence scores and does not include spatial coordinates, detect brands focuses on identifying brand logos rather than general objects and does not provide full object location details, and recognize domain-specific content is intended for specialized scenarios and is deprecated, making it unsuitable for detecting and locating multiple objects in an image.",
    "references": [
      "https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/concept-object-detection"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 69,
    "question_text": "You build an app named App1 that uses the Azure Face in Foundry Tools service.\nYou need to optimize the app for images that contain blurry faces.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Change the recognition model to recognition_02.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Decrease the faceIdTimeToLive value.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Set the detection model to detection_03.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Model_3 Improves accuracy on small, side-view, and blurry faces.\nChanging the recognition model to recognition_02 will improve facial recognition and faceIdTimeToLive is used for the number of seconds that the face ID is cached, which has no impact on blurry faces.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/computer-vision/how-to/specify-detection-model",
      "https://learn.microsoft.com/training/modules/detect-analyze-recognize-faces/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 70,
    "question_text": "You are building an app that will be deployed to an edge device and will use Azure AI Custom Vision to analyze images of fruits.\nYou need to select a model domain for the app. The solution must support running the app without internet connectivity.\nWhich model should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Compact domain",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Food domain",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "General[A1] domain",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "General [A2] domain",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Only Compact domain is correct. The Azure AI Custom Vision service only exports compact domains, and the models generated by compact domains are optimized for the constraints of real-time classification on mobile devices.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/custom-vision-service/select-domain",
      "https://learn.microsoft.com/azure/cognitive-services/custom-vision-service/export-your-model",
      "https://learn.microsoft.com/training/modules/classify-images/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 71,
    "question_text": "You are building an app that uses Azure AI Video Indexer.\nYou need to extract keyframes from uploaded video and store them on a disk by using the API.\nHow should you implement the solution?",
    "options": [
      {
        "letter": "a",
        "text": "Upload the video and download the ZIP file of the thumbnails.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Upload the video, download the ZIP file of the thumbnails, and get the thumbnail for each keyframe.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Upload the video, get the video index, and get the thumbnail for each keyframe.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "You need to upload the video, get the video index, and get the thumbnail for each keyframe. Three API calls need to be done.\nUploading the video and then downloading the ZIP file of the thumbnails is the path through the Azure portal. You need the index to know the correct parameters for the thumbnail request.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/analyze-video/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 72,
    "question_text": "You are using a custom Language content model in an Azure AI Video Indexer solution.\nDuring testing, you upload a text file that includes the following sentence: “Kubernetes is a new feature in Azure & the cloud.”\nThe sentence is discarded.\nYou need to ensure that the model retains the sentence.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Change the model to a custom slate detection model.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Remove the “&” character from the text file.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Retrain the model.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "You need to remove the “&” character because sentences with special characters will be discarded.\nKubernetes is highly specific and unknown to the model, so retraining the model is incorrect. The slate model is for clapper boards and digital patterns with color bars.",
    "references": [
      "https://learn.microsoft.com/azure/azure-video-indexer/customize-language-model-overview",
      "https://learn.microsoft.com/training/modules/analyze-video/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 73,
    "question_text": "Your team is deploying an agent using the Microsoft Foundry Agent Service to perform actions based on user requests, such as scheduling meetings and sending notifications.\nYou need to integrate the AI agent with tools that enable it to perform these actions programmatically.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Add custom functions.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Configure static templates.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Enable default model capabilities.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use a pre-trained chatbot framework.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Adding custom functions enables the AI agent to dynamically execute specific actions such as scheduling meetings and sending notifications, making it the most suitable choice for the scenario. Configuring static templates does not provide the flexibility needed for dynamic action execution, limiting its effectiveness. Default model capabilities lack the ability to perform specific programmatic actions, making them insufficient for the task. Using a pre-trained chatbot framework without customization fails to address the specific requirements for executing actions programmatically, reducing its applicability in this context.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/build-agent-with-custom-tools/"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 74,
    "question_text": "Your organization plans to develop an AI solution using Microsoft Microsoft Foundry Agent Service to automate customer support workflows.\nYou need to configure the agent to interact with external data sources and execute actions based on user queries.\nEach correct answer presents part of the solution. Which two actions should you perform?",
    "options": [
      {
        "letter": "a",
        "text": "Add Bing search tools in the agent configuration.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Configure Azure Functions in the agent configuration file.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Deploy a containerized image.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Set up a SQL database.",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "Use Computer Vision.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a",
      "b"
    ],
    "explanation": "Adding Bing search tools enables the agent to retrieve relevant information from external sources, enhancing its ability to respond accurately to user queries. Configuring Azure Functions allows the agent to execute specific actions programmatically, which is crucial for automating workflows based on user inputs. Deploying a containerized image is not necessary to configure the agent's capabilities. Setting up a SQL database is not required for configuring the agent's interaction capabilities, as it does not directly contribute to the goal of automating customer support workflows. Using Computer Vision is unrelated to the task of configuring the agent for customer support workflows, as it focuses on image analysis rather than interaction with external data sources.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/develop-ai-agent-azure/"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 75,
    "question_text": "You are building a solution that uses Azure AI Search.\nYou need to create a skillset definition.\nWhat are minimum sections you should include in the definition?",
    "options": [
      {
        "letter": "a",
        "text": "name, description, and skills",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "name, description, knowledgeStore, and encryptionKey",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "name, description, skills, and cognitiveServices",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "name, description, skills, knowledgeStore, and encryptionKey",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Only name, description, and skills are required.\ncognitiveServices is used for billable skills that call Cognitive Services APIs.\nknowledgeStore specifies an Azure Storage account and the settings for projecting the skillset output into tables, blobs, and files in Azure Storage.\nencryptionKey specifies an Azure key vault and customer-managed keys used to encrypt sensitive content (descriptions, connection strings, keys) in a skillset definition.",
    "references": [
      "https://learn.microsoft.com/azure/search/cognitive-search-defining-skillset#add-a-skillset-definition",
      "https://learn.microsoft.com/training/modules/create-enrichment-pipeline-azure-cognitive-search/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 76,
    "question_text": "You are building a knowledge mining solution that will use AI enrichment and Azure AI Search.\nYou need to create a data structure that will be used to store the enriched and indexed output for downstream apps.\nWhat should you create?",
    "options": [
      {
        "letter": "a",
        "text": "a knowledge store",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "a searchable index",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "a searchable store",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "an enrichment cache",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "A knowledge store is used for downstream apps, such as knowledge mining and data science. A knowledge store is defined within a skillset. Its definition determines whether your enriched documents are projected as tables or objects (files or blobs) in Azure Storage.",
    "references": [
      "https://learn.microsoft.com/azure/search/cognitive-search-concept-intro",
      "https://learn.microsoft.com/training/modules/create-knowledge-store-azure-cognitive-search/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 77,
    "question_text": "You are building a knowledge mining solution that uses Azure AI Search.\nYou need to extract content from a file within the enrichment pipeline by using AI enrichment.\nWhich built-in skill should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Microsoft.Skills.Text.KeyPhraseExtractionSkill",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Microsoft.Skills.Text.SplitSkill",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Microsoft.Skills.Text.V3.EntityRecognitionSkill",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Microsoft.Skills.Util.DocumentExtractionSkill",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Microsoft.Skills.Util.DocumentExtractionSkill is the built-in skill used to extract content from a file within the enrichment pipeline.",
    "references": [
      "https://learn.microsoft.com/azure/search/cognitive-search-predefined-skills",
      "https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 78,
    "question_text": "Your company processes various forms, including health insurance cards and identity documents, and requires accurate extraction of specific fields.\nYou need to recommend a solution using Azure Document Intelligence in Foundry Tools for extracting structured data from these forms.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Train a composed model.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Use Azure OCR.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Use prebuilt models for Health Insurance Card and ID Document.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Use the Layout model.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Prebuilt models for Health Insurance Card and ID Document in Microsoft Azure Document Intelligence in Foundry Tools are the optimal choice for extracting structured data from these forms because they are specifically designed for this purpose, ensuring accuracy and efficiency. Training a composed model introduces unnecessary complexity when prebuilt models are available for these document types. Azure OCR focuses on text extraction without contextual understanding, making it unsuitable for structured data extraction. The Layout model supports key-value pair extraction but lacks the specificity required for extracting specific fields from health insurance cards and identity documents.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/plan-form-recognizer-solution/2-understand",
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/3-get-started"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 79,
    "question_text": "Your company receives incoming documents that need to be classified into predefined categories before extracting specific data fields.\nYou need to implement a solution that accurately classifies documents and supports subsequent data extraction.\nWhich solution should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Vision in Foundry Tools OCR (Optical Character Recognition)",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Custom classification model in Azure Document Intelligence in Foundry Tools",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Custom template model in Azure Document Intelligence in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Invoice model in Azure Document Intelligence in Foundry Tools",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "The custom classification model in Azure Document Intelligence in Foundry Tools is the most suitable solution because it is specifically designed to categorize documents into predefined categories, enabling accurate classification and supporting subsequent data extraction. Custom template models are limited to data extraction from structured documents and do not address classification needs. Azure Vision in Foundry Tools OCR focuses solely on text recognition and lacks the functionality required for document categorization. The Invoice model is tailored for invoice processing and does not meet the broader requirements for document classification described in the scenario.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/2-what-form-recognizer",
      "https://learn.microsoft.com/en-us/training/modules/use-prebuilt-form-recognizer-models/4-use-financial-id-tax-models"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 80,
    "question_text": "Your organization scans and stores various document types, such as invoices, receipts, and custom forms, in PDF format.\nYou need to extract structured information from these documents for analysis.\nEach correct answer presents part of the solution. Which two actions should you take?",
    "options": [
      {
        "letter": "a",
        "text": "Convert the documents into plain text.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Train a custom model using labeled datasets in Azure Document Intelligence in Foundry Tools.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Use the General Document model in Azure Document Intelligence in Foundry Tools.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use the Layout model in Azure Document Intelligence in Foundry Tools.",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "Use the prebuilt Invoice model in Azure Document Intelligence in Foundry Tools.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "b",
      "e"
    ],
    "explanation": "Converting the documents into plain text results in the loss of structural information, which is essential for extracting meaningful data. Training a custom model using labeled datasets in Azure Document Intelligence in Foundry Tools enables accurate extraction of data from unique forms that do not conform to standard templates. The General Document model is deprecated and not recommended for use in new solutions, making it unsuitable for this task. The Layout model can extract text and structure but is not optimized for specific document types like invoices or receipts, which limits its effectiveness for this scenario. The prebuilt Invoice model in Azure Document Intelligence in Foundry Tools is specifically designed to extract key information from invoices, making it an appropriate choice for this task.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/3-get-started",
      "https://learn.microsoft.com/en-us/training/modules/plan-form-recognizer-solution/4-choose-model-type"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 81,
    "question_text": "You use an Azure OpenAI in Foundry Models service to generate content for customer support chatbots. The deployed model is set to auto-update to a default version.\nYou need to ensure consistent chatbot behavior during model updates and test new versions before deployment.\nWhich two actions should you take to meet these requirements? Each correct answer presents part of the solution.",
    "options": [
      {
        "letter": "a",
        "text": "Disable automatic updates.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Enable updates to the default version.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Increase deployment resources.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Select a specific model version.",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "Test new versions separately.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d",
      "e"
    ],
    "explanation": "Disabling automatic updates results in outdated models, which negatively impact performance and security. Enabling updates to the default version applies new updates without prior testing, leading to potential inconsistencies in chatbot behavior. Increasing deployment resources does not address consistent chatbot behavior or testing of new model versions. Selecting a specific model version ensures consistent chatbot behavior by allowing manual control over updates and testing. Testing new versions in a separate deployment identifies and resolves issues before they affect the main chatbot, ensuring controlled updates and consistent behavior.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/generate-images-azure-openai/3-dall-e-in-openai-studio",
      "https://learn.microsoft.com/en-us/training/modules/monitor-ai-services/2-monitor-cost"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 82,
    "question_text": "Your organization is using an Azure OpenAI in Foundry Models service for document summarization across various document types.\nYou need to ensure the AI generates summaries that meet organizational requirements.\nWhat action should you take to achieve this?",
    "options": [
      {
        "letter": "a",
        "text": "Enable diagnostic logging.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Increase the token limit.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Refine prompts to specify key details.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Switch to a higher-cost model.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Refining prompts to specify key details ensures the AI model generates concise and relevant summaries, directly addressing the optimization requirement. Enabling diagnostic logging provides insights into system performance but does not impact the quality of generated summaries. Increasing the token limit allows for longer responses but does not inherently improve quality. Switching to a higher-cost model may enhance performance but does not guarantee better summaries without prompt optimization.",
    "references": [
      "https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering?tabs=chat"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 83,
    "question_text": "You are creating a model to detect animals in wildlife images.\nYou need to train a model that identifies and locates animals efficiently.\nWhat should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Build a CNN model by using Azure Machine Learning.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Train a classification model by using Azure Custom Vision.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Use Azure Custom Vision to train an object detection model.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Use the prebuilt image tagging model in Azure Vision in Foundry Tools.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Using Azure Custom Vision to train an object detection model is the optimal choice because it meets the requirements for identifying and locating animals in wildlife images. Building a CNN model by using Azure Machine Learning is resource-intensive and unnecessary given the availability of Azure Custom Vision. Training a classification model by using Azure Custom Vision does not provide localization information, which is essential for this task. Using the prebuilt image tagging model in Azure Vision in Foundry Tools lacks customization and localization capabilities, making it unsuitable for this scenario.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/enrich-search-index-using-language-studio/03-enrich-cognitive-search-index-custom-classes",
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/3-get-started"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 84,
    "question_text": "You are building an app that will use the Azure AI Custom Vision API to detect when all the spaces in a parking lot are empty.\nWhich feature of the API should you use?",
    "options": [
      {
        "letter": "a",
        "text": "image classification",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "image description",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "object detection",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Object detection is similar to image classification, but it returns the coordinates in an image where the applied label(s) can be found.\nImage description analyzes an image and generates a human-readable phrase that describes its contents. Image classification applies one or more labels to an entire image.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/custom-vision-service/overview",
      "https://learn.microsoft.com/training/modules/detect-objects-images/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 85,
    "question_text": "You are building a video processing app that will use Azure AI Video Indexer to extract insights from videos that contain multi-language content.\nYou need to configure the API calls to enable multilingual identification.\nWhich value should you set for the sourceLanguage parameter?",
    "options": [
      {
        "letter": "a",
        "text": "language detection",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "multi detection",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "multi-language detection",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "multi-lingual detection",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "When indexing or reindexing a video by using the API, choose the multi-language detection option for the sourceLanguage parameter. The remaining options do not configure the API calls to enable multilingual identification.",
    "references": [
      "https://learn.microsoft.com/azure/azure-video-indexer/multi-language-identification-transcription",
      "https://learn.microsoft.com/training/modules/analyze-video/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 86,
    "question_text": "You are building an app that will flag documents that contain the names of staff members by using the Azure Language in Foundry Tools Personally Identifiable Information (PII) detection feature.\nYou need to configure the PII detection feature.\nWhich category should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Age",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "DateTime",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Person",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "PhoneNumber",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "The Person category detects names of people in the PII detection feature. The PhoneNumber category detects phone numbers, the age category detects people’s ages, and the DateTime detects dates and time values.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/personally-identifiable-information/concepts/entity-categories",
      "https://learn.microsoft.com/training/modules/publish-use-language-understand-app/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 87,
    "question_text": "You have a custom question answering project in Azure Language in Foundry Tools Service\nA customer asks a question that is not part of the project.\nYou review the active learning suggestions and do not see any suggestions.\nYou need to ensure that customer questions are included in the active learning suggestions. The solution must minimize administrative effort.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Add the customer questions to the editor manually.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Configure active learning.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Enable logging for the project.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Wait at least 30 minutes before checking for suggestions.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Active learning suggestions are not in real time. There is an approximate delay of 30 minutes before suggestions show on this pane. This delay balances the high cost involved in real-time updates to the index and service performance.\nActive learning is turned on by default. You can use active learning for this instead of manually logging the questions and adding them.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/question-answering/tutorials/active-learning",
      "https://learn.microsoft.com/training/modules/build-qna-solution-qna-maker/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 88,
    "question_text": "You are building an app that will extract text from scanned receipts.\nYou need to recommend which service to use. The solution must minimize development effort.\nWhat should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Vision in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Azure AI Custom Vision",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure Document Intelligence in Foundry Tools",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Azure Machine Learning",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Azure Document Intelligence in Foundry Tools is designed to work with documents such as receipts, as it offers prebuilt models for extracting information from these kinds of documents.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview-ocr",
      "https://learn.microsoft.com/training/modules/work-form-recognizer/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 89,
    "question_text": "You are building an app that will identify the core concepts of a document by using Azure AI language.\nWhich endpoint should you use as part of the solution?",
    "options": [
      {
        "letter": "a",
        "text": "custom Named Entity Recognition (NER)",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "key phrase extraction",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "the Azure Vision in Foundry Tools API",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "You should use the key phrase extraction endpoint.\nThe custom NER endpoint will not do key phrase extraction and the Azure Vision in Foundry Tools API can be used to process PDF files but not to extract key phrase detection.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/key-phrase-extraction/overview",
      "https://learn.microsoft.com/training/modules/extract-insights-text-with-text-analytics-service/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 90,
    "question_text": "You are building an image moderation solution based on Microsoft Foundry Content Safety.\nYou migrate images to an Azure Blob Storage location.\nYou need to configure an access solution to enable the Foundry Content Safety solution to analyze the images.\nWhich two actions should you perform? Each correct answer presents part of the solution.",
    "options": [
      {
        "letter": "a",
        "text": "Assign the Storage Blob Data Contributor/Owner/Reader role to the user-assigned managed identity.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Assign the Storage Blob Data Contributor/Owner/Reader role to the system-assigned managed identity.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Enable a system-assigned managed identity for the Content Safety instance.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Enable a user-assigned managed identity for the Content Safety instance.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b",
      "c"
    ],
    "explanation": "The only way to provide a Content Safety resource with access to images in Azure Blob Storage, is to enable a system-assigned managed identity and to assign the role Storage Blob Data Contributor/Owner/Reader to that managed identity. User-assigned managed identities are not permitted in that scenario.",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/content-safety/quickstart-image?tabs=visual-studio%2Cwindows&pivots=programming-language-rest"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 91,
    "question_text": "You are developing a containerized optical character recognition (OCR)-capable application by using Azure AI Services containers.\nWhile developing the solution, you retrieve a status message of “Mismatch”, and the connection to the AI Services resource fails.\nYou need to ensure that the solution can connect to the AI Services resource.\nWhat should you do?",
    "options": [
      {
        "letter": "a",
        "text": "Confirm that the API key is for the correct region.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Confirm that the API key is for the correct resource type.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Confirm that the Azure AI Services resource is online.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Upgrade the Azure AI Services resource to a higher tier.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "Mismatch means that the wrong API key has been used. If you have provided an API key or endpoint for a different kind of Azure AI Services resource, you find your API key and service region in the Azure portal, in the Keys and Endpoint section for your Azure AI Services resource.\nIf the API key is invalid, you must confirm that the API key is for the correct region. If the API key has exceeded the quota, then you can either upgrade your pricing tier or wait for an additional quota to become available. Find your tier in the Azure portal, in the Pricing Tier section of your Azure AI Service resource. The mismatch error would not be generated if the resource was offline.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/containers/container-faq",
      "https://learn.microsoft.com/training/modules/investigate-container-for-use-cognitive-services/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 92,
    "question_text": "You create a Microsoft Foundry Content Safety solution to monitor the text input of a chat forum.\nYou discover that the solution returns a severity level of 0 for various instances of harmful text.\nWhich type of accuracy error is returned by the solution?",
    "options": [
      {
        "letter": "a",
        "text": "a false negative",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "a false positive",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "a true negative",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "a true positive",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "A false negative arises when the model fails to identify harmful content and returns a severity level of 0. There are four accuracy error types, and a false negative indicates incorrectly accepted content in the text.",
    "references": [
      "https://learn.microsoft.com/legal/cognitive-services/content-safety/transparency-note?context=%2Fazure%2Fai-services%2Fcontent-safety%2Fcontext%2Fcontext"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 93,
    "question_text": "Your organization is developing an AI agent for customer support inquiries that must integrate with Azure resources.\nYou need to select a framework that supports AI orchestration and multi-agent workflows.\nWhat should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Bot Framework",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Azure Machine Learning Studio",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Cognitive Services API",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Semantic Kernel framework",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Azure Bot Framework is commonly used for building conversational bots but does not provide the orchestration features required for multi-agent workflows. Azure Machine Learning Studio is focused on developing machine learning models and does not address the need for orchestration in this scenario. Cognitive Services API offers pre-built AI functionalities but lacks the ability to orchestrate multi-agent workflows. Semantic Kernel framework is the most suitable choice as it supports generative AI orchestration and multi-agent workflows, aligning perfectly with the requirements of the scenario.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/analyze-images/2-provision-computer-vision-resource",
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/5-tools-and-sdks"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 94,
    "question_text": "You are deploying an Azure OpenAI in Foundry Models service.\nYou plan to use your own data in the models you will deploy.\nYou need to ensure that the model can index your data sources.\nWhich additional Azure service should you deploy?",
    "options": [
      {
        "letter": "a",
        "text": "Azure AI Search",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Content Moderator",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Language",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Personalizer",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Azure OpenAI on your data enables developers to use supported AI chat models that can reference specific sources of information to ground the response. Adding this information allows the model to reference both the specific data provided and its pretrained knowledge to provide more effective responses. Azure OpenAI on your data utilizes the search ability of Azure AI Search to add the relevant data chunks to a prompt.\nAzure OpenAI on your data still uses a stateless API to connect to the model, which removes the requirement of training a custom model with your data and simplifies the interaction with the AI model. Cognitive Search first finds the useful information to answer the prompt, and Azure OpenAI forms the response based on that information.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/build-copilot-ai-studio/",
      "https://learn.microsoft.com/azure/ai-services/what-are-ai-services"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 95,
    "question_text": "Your company processes scanned invoices to automate financial recordkeeping.\nYou need to identify and extract key fields from these invoices.\nEach correct answer presents part of the solution. Which three actions should you take?",
    "options": [
      {
        "letter": "a",
        "text": "Ensure input files meet required specifications.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Select the Business Card model.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Select the Invoice model.",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Select the Read model.",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "Use low-quality images.",
        "is_correct": false
      },
      {
        "letter": "f",
        "text": "Use text-based PDF files.",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "a",
      "c",
      "f"
    ],
    "explanation": "Meeting input requirements, such as file format and resolution, is essential for optimal data extraction performance. Text-embedded PDFs eliminate errors in character recognition, enhancing the accuracy of data extraction and optimizing the OCR process for structured data extraction. The Microsoft Invoice model is specifically trained to extract structured data from invoices, including fields like invoice number and total amount, directly addressing the requirement for structured data extraction from invoices. Low-resolution images reduce the accuracy of data extraction, making them unsuitable for this task. The Microsoft Business Card model is tailored for extracting data from business cards, not invoices, and does not align with the goal of extracting structured data from invoices. The Microsoft Read model is designed for general text extraction and does not specialize in structured data extraction from invoices, failing to meet the specific requirements of the task.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/use-prebuilt-form-recognizer-models/2-understand-prebuilt-models",
      "https://learn.microsoft.com/en-us/training/modules/plan-form-recognizer-solution/2-understand"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 96,
    "question_text": "You have a website that allows users to upload images.\nYou need to ensure that the uploaded images do not contain adult content. The solution must minimize development effort.\nWhich service should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Face in Foundry Tools service",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Azure AI Custom Vision",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure Vision in Foundry Tools Image Analysis",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "Azure Vision in Foundry Tools Spatial Analysis",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "The Azure Vision in Foundry Tools Image Analysis service can extract a wide variety of visual features from an image. One of them is to detect adult content.\nThe Azure AI Face service provides AI algorithms that detect, recognize, and analyze human faces in images. Azure AI Custom Vision is an image recognition service that lets you build, deploy, and improve own image identifier models. So, while it is possible, it is not the solution with the lowest development effort. Azure Vision in Foundry Tools Spatial Analysis is used to ingest streaming video from cameras, extract insights, and generate events to be used by other systems.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/what-are-cognitive-services",
      "https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview",
      "https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview-image-analysis?tabs=3-2",
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 97,
    "question_text": "You are building an app that will use a Microsoft Foundry Service resource.\nYou need to identify the endpoint for the resource.\nFrom the Azure CLI, which command should you run?",
    "options": [
      {
        "letter": "a",
        "text": "az cognitiveservices account show",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "az cognitiveservices account show --name myresource",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "az cognitiveservices account show --name myresource --resource-group cognitive-services-resource-group",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "az cognitiveservices account show --resource-group cognitive-services-resource-group",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "As you need to provide the name and the resource group for your Cognitive Service account to retrieve the endpoint amongst other information for the resource, az cognitiveservices account show --name myresource --resource-group cognitive-services-resource-group is the only valid command.",
    "references": [
      "https://learn.microsoft.com/cli/azure/cognitiveservices/account?view=azure-cli-latest#az-cognitiveservices-account-show",
      "https://learn.microsoft.com/training/modules/create-manage-cognitive-services/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 98,
    "question_text": "You have a Microsoft Foundry Service resource.\nYou need to enable diagnostic logging.\nWhat are two prerequisites for diagnostic logging? Each correct answer presents a complete solution.",
    "options": [
      {
        "letter": "a",
        "text": "a Log Analytics workspace",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "an Azure Cosmos DB for NoSQL account",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "an Azure key vault",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "an Azure SQL database",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "an Azure Storage account",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "a",
      "e"
    ],
    "explanation": "The prerequisites to enable diagnostic logging are to have an Azure Storage resource that retains diagnostic logs for policy audit, static analysis, or backup. A Log Analytics resource provides a flexible log search and analytics tool that allows for analysis of raw logs generated by an Azure resource.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/diagnostic-logging",
      "https://learn.microsoft.com/training/modules/monitor-cognitive-services/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 99,
    "question_text": "You are building a custom agent based on Azure OpenAI.\nYou discover attempts to exploit the copilot by jailbreaking it through a User Prompt Injection Attack (UPIA).\nYou need to increase security to prevent the jailbreak attack.\nWhich Microsoft Foundry Service should you include in the solution?",
    "options": [
      {
        "letter": "a",
        "text": "Content Safety",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Face",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Language",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Video Indexer",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "Vision",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Content Safety jailbreak risk detection recognizes four different classes of jailbreak attacks: Attempt to change system rules, embedding a conversation mockup to confuse the model, role-play, and encoding attacks. These can be used in LLM-based applications to prevent jailbreak attacks. All the other services listed are designed for different purposes.",
    "references": [
      "https://learn.microsoft.com/azure/ai-services/content-safety/concepts/jailbreak-detection",
      "https://learn.microsoft.com/azure/ai-services/what-are-ai-services"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 100,
    "question_text": "You are building an app that will analyze meeting recordings and identify who is speaking at which moment in time.\nYou need to configure a voice profile for the app.\nWhich type of voice profile should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Speaker identification",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "text-dependent verification",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "text-independent verification",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Text-independent verification means that speakers can speak in everyday language in enrollment and verification phases.\nText-dependent verification means that speakers need to choose the same passphrase to use during both enrollment and verification phases. This is the voice profile that should be used when configuring a voice profile for the app.\nSpeaker identification helps you determine an unknown speaker’s identity within a group of enrolled speakers.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/speech-service/speaker-recognition-overview",
      "https://learn.microsoft.com/training/modules/transcribe-speech-input-text/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 101,
    "question_text": "You are building a model that uses Conversational Language Understanding (CLU).\nYou need to measure how accurate the model is by using the ratio between the correctly identified positives (true positives) and all identified positives.\nWhich metric should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Bilingual Evaluation Understudy (BLEU)",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "F1 score",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "precision",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "recall",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Precision measures how precise/accurate a model is. It is the ratio between the correctly identified positives (true positives) and all identified positives. The precision metric reveals how many of the predicted classes are correctly labeled.\nRecall measures the model's ability to predict actual positive classes. F1 score is a function of precision and recall. BLEU is from the Azure Translator in Foundry Tools service.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/conversational-language-understanding/concepts/evaluation-metrics",
      "https://learn.microsoft.com/training/modules/build-language-understanding-model/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 102,
    "question_text": "You are building a custom translation model.\nYou need to use bilingual training documents to teach the model your terminology and style.\nWhich rule should you follow?",
    "options": [
      {
        "letter": "a",
        "text": "Be liberal.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Be restrictive.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Be strict.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Be liberal is correct. Any in-domain human translation is better than machine translation. Add and remove documents as you go and try to improve the Bilingual Evaluation Understudy (BLEU) score.\nBe strict is incorrect. Compose them to be optimally representative of what you are going to translate in the future. Be restrictive is also incorrect. A phrase dictionary is case-sensitive, and any word or phrase listed is translated in the way you specify. In many cases, it is better to not use a phrase dictionary and let the system learn.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/translator/custom-translator/beginners-guide#what-should-i-use-for-training-material",
      "https://learn.microsoft.com/training/modules/translate-text-with-translator-service/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 103,
    "question_text": "You are building a custom translation model.\nYou need to evaluate the precision of the text that you translated by using a Bilingual Evaluation Understudy (BLEU) score.\nWhich scale is used for the score?",
    "options": [
      {
        "letter": "a",
        "text": "between 0 and 1",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "between 0 and 100",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "low, medium, and high",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "A BLEU score is a number between zero and 100. A score of zero indicates a low-quality translation, where nothing in the translation matches the reference. A score of 100 indicates a perfect translation that is identical to the reference. It is unnecessary to attain a score of 100. A BLEU score between 40 and 60 indicates a high-quality translation.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/translator/custom-translator/beginners-guide#what-is-a-bleu-score",
      "https://learn.microsoft.com/training/modules/translate-text-with-translator-service/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 104,
    "question_text": "You are building an app named App1 that uses the Image Analysis API.\nYou are evaluating analyzing images by using the following request.\nhttps://*.cognitiveservices.azure.com/computervision/imageanalysis:analyze? features=read,description\nWhich results will the request return?",
    "options": [
      {
        "letter": "a",
        "text": "a description of the image content only",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "the visible text in the image and a description of the image content",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "The features used in the call are read and description, which will return the visible text in the image, as well as a description of the image content.\nTo return the objects that are in the image and their approximate location, the feature used in the call should be objects. To return a description of the image content only, the feature description should be used alone.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/computer-vision/how-to/call-analyze-image?tabs=rest",
      "https://learn.microsoft.com/training/modules/read-text-images-documents-with-computer-vision-service/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 105,
    "question_text": "You are building an app that will use Azure AI Custom Vision to analyze and classify images to build an image library of animals.\nYou need to configure the classification type for the Azure AI Custom Vision project. The solution must ensure that the images selected only include a single animal.\nWhich type of classification should you use?",
    "options": [
      {
        "letter": "a",
        "text": "multiclass",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "multilabel",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "singleclass",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "singlelabel",
        "is_correct": false
      },
      {
        "letter": "e",
        "text": "unilabel",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Multilabel classification applies any number of tags to an image (zero or more), while multiclass classification sorts images into single categories (every image you submit will be sorted into the most likely tag). Therefore, using multilabel means that one image can be tagged as both “cat” and “dog” at the same time, although it should be either/or.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/custom-vision-service/getting-started-build-a-classifier",
      "https://learn.microsoft.com/training/modules/classify-images/"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 106,
    "question_text": "You are building an app that uses the Azure AI Video indexer API to analyze Microsoft Teams meeting recordings. The app will search for images and mentions of competing companies.\nWhich content model should you use?",
    "options": [
      {
        "letter": "a",
        "text": "custom brands",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "custom Language",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "custom slate",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "The Custom Brands model supports brand detection from speech and visuals.\nSlate detection is used for clapper boards and digital patterns with color bars, and the custom Language model is used to add words that are not in the model.",
    "references": [
      "https://learn.microsoft.com/azure/azure-video-indexer/customize-brands-model-overview",
      "https://learn.microsoft.com/training/modules/analyze-video/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 107,
    "question_text": "You are building a solution that uses Azure AI Search.\nYou need to execute the initial run of the indexer.\nWhich stages will be included during the initial run?",
    "options": [
      {
        "letter": "a",
        "text": "connecting to an Azure data source, creating an index schema, and running the wizard to create objects and load data",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "creating a data source, creating an index, and creating and running the indexer",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "document cracking, field mapping, skillset execution, and output field mapping",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "Document cracking, field mapping, skillset execution, and output field mapping are the stages of indexing.\nCreating a data source, creating an index, and creating and running the indexer are the stages to create an indexer. Connecting to an Azure data source, creating an index schema, and running the wizard to create objects and load data are the stages for the Import Data wizard.",
    "references": [
      "https://learn.microsoft.com/azure/search/search-indexer-overview#stages-of-indexing",
      "https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 108,
    "question_text": "You have an Azure subscription that contains an Azure AI services resource named AIService1.\nYou need to extract information from content, such as documents, images, videos, and audio. The solution must minimize development effort.\nWhat should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Content Understanding in Foundry Tools",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Azure Document Intelligence in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure Language in Foundry Tools",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Azure Machine Learning",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "You should use Azure Content Understanding in Foundry Tools because it is a multimodal service designed to extract and structure information from a wide range of content types, including documents, images, videos, and audio, through a unified and simplified development experience. It minimizes development effort by providing a single service and consistent APIs for analyzing diverse content formats. Azure Document Intelligence is limited to document-centric scenarios, Azure Language focuses only on text analysis, and Azure Machine Learning requires custom model development and training, which increases complexity and development effort compared to using a purpose-built content understanding service.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/analyze-content-ai/01-introduction",
      "https://learn.microsoft.com/en-us/training/modules/analyze-content-ai/02-content-understanding"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 109,
    "question_text": "Your company needs to analyze images of retail shelves to identify product placement and stock levels.\nYou need to implement a Microsoft Azure AI solution to analyze these images.\nWhat should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Train an Azure AI Custom Vision model.",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Use Azure Content Understanding in Foundry Tools.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Use an Azure Vision in Foundry Tools prebuilt model.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use a Microsoft Foundry GPT model.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "Azure Content Understanding in Foundry Tools is the most appropriate solution as it is specifically designed to analyze images and extract structured data, such as product counts, directly addressing the requirements. Training a Custom Vision model would require additional effort and resources without providing any significant advantage over the existing capabilities of Azure Content Understanding in Foundry Tools. Prebuilt Azure Vision in Foundry Tools models can detect bounding boxes but does not offer the ability to count or extract structured data about products, making it unsuitable for this task. Microsoft Foundry GPT models can provide information based on provided data or information but is unable to achieve the desired result on its own.",
    "references": [
      "https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/image/overview"
    ],
    "category": "Implement computer vision solutions"
  },
  {
    "question_number": 110,
    "question_text": "You have a set of JPEG, PNG, and TIFF files that contain photos of documents. The documents contain structured data in tables.\nYou plan to use the layout model in Azure Document Intelligence in Foundry Tools to extract the tables from the files.\nYou need to extract the tables from the files by using the layout model. The solution must NOT require model training.\nWhich three methods can you use? Each correct answer presents a complete solution.",
    "options": [
      {
        "letter": "a",
        "text": "a client library SDK",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "a prebuilt model",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "a REST API",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "the Document Intelligence Studio",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "a",
      "c",
      "d"
    ],
    "explanation": "You can extract tables from document images by using the Azure Document Intelligence layout model without any model training by invoking it through supported access methods, including a client library SDK, the REST API, or the Document Intelligence Studio. The client library SDK and REST API allow you to programmatically call the prebuilt layout model (modelId = prebuilt-layout) to extract tables and structural information from JPEG, PNG, and TIFF files, while Document Intelligence Studio provides a no-code, graphical experience to upload documents and run the same layout model interactively. A prebuilt model itself is not a method of extraction but rather the type of model being used, so it does not represent a complete solution on its own.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/2-what-form-recognizer",
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/9-form-recognizer-studio",
      "https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept/retrieval-augmented-generation?view=doc-intel-4.0.0"
    ],
    "category": "Implement knowledge mining and information extraction solutions"
  },
  {
    "question_number": 111,
    "question_text": "You are building a mobile app that will enable users to scan street signs and will read out the text on the sign.\nYou need to recommend a service to use. The solution must minimize development effort.\nWhich service should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Vision in Foundry Tools",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Azure AI Custom Vision",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure Face in Foundry Tools service",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Azure Document Intelligence in Foundry Tools",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Azure Vision in Foundry Tools is the only service which can achieve the desired result.\nAzure AI Custom Vision and Azure AI Face do not offer OCR. Azure Document Intelligence in Foundry Tools is designed for documents, but not images.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview-ocr",
      "https://learn.microsoft.com/training/modules/read-text-images-documents-with-computer-vision-service/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 112,
    "question_text": "You have a Microsoft Foundry service.\nYou plan to create an agent that will automate the sending of expense report emails to employees. The solution will use multiple types of agents to support the workflow.\nYou need to create the agent while minimizing administrative effort.\nWhat should you use?",
    "options": [
      {
        "letter": "a",
        "text": "the Azure AI Agent Service SDK",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "the Microsoft Foundry portal",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "the Microsoft Foundry REST API",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "the Microsoft Foundry SDK",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "b"
    ],
    "explanation": "The Microsoft Foundry portal is the best choice because it provides a graphical, no-code interface to create, configure, and test multiple types of agents directly within the Foundry service, which minimizes administrative and development effort. The Microsoft Foundry SDK and Azure AI Agent Service SDK are designed for developers who need programmatic control and advanced orchestration, which increases setup and maintenance overhead, while the Microsoft Foundry REST API requires scripting and endpoint management, making it less suitable when the goal is to quickly create and manage agents with minimal administrative effort.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/ai-foundry-sdk/01-introduction",
      "https://learn.microsoft.com/en-us/training/modules/develop-ai-agent-with-semantic-kernel/1-introduction"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 113,
    "question_text": "Your organization is implementing Microsoft Foundry Content Safety to moderate user-generated content on a social messaging platform.\nYou need to configure Foundry Content Safety to identify and restrict harmful text content, including profanities and hate speech, while allowing customization for specific patterns.\nEach correct answer presents part of the solution. Which two actions should you take?",
    "options": [
      {
        "letter": "a",
        "text": "Configure harmful patterns using the Custom Categories API.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Enable built-in blocklists in Content Safety Studio.",
        "is_correct": true
      },
      {
        "letter": "c",
        "text": "Enable encryption at rest with customer-managed keys.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use the Analyze Image API for text analysis.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a",
      "b"
    ],
    "explanation": "Enabling built-in blocklists in Content Safety Studio is essential as it provides predefined terms to effectively flag harmful content such as profanities and hate speech. Configuring harmful patterns using the Custom Categories API allows for tailored moderation by defining specific patterns that align with organizational needs. Enabling encryption at rest with customer-managed keys enhances data security but does not contribute to the detection or restriction of harmful text content. Using the Analyze Image API focuses on image moderation and is not applicable to the task of moderating text content.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/3-azure-ai-services",
      "https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/9-form-recognizer-studio"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 114,
    "question_text": "You are developing an AI agent using Microsoft Foundry Agent Service to assist with data retrieval and analysis tasks.\nYou need to configure the AI agent to interact effectively with external data sources.\nWhat should you use?",
    "options": [
      {
        "letter": "a",
        "text": "Configure Azure AI Search.",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "Deploy a custom model in Azure Machine Learning.",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Implement Microsoft Bot Framework.",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Use Azure Document Intelligence in Foundry Tools.",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Configuring Azure AI Search is the most appropriate solution because it provides indexing and querying capabilities that enable effective interaction with external data sources. Deploying a custom model in Azure Machine Learning is not suitable as it focuses on model deployment rather than facilitating external data source interaction. Using Azure Document Intelligence in Foundry Tools is not appropriate because it is designed for extracting structured data from documents, which does not align with the broader goal of data retrieval and analysis. Implementing Azure Bot Framework is ineffective in this context as it is intended for building conversational interfaces and does not address the need for data retrieval and analysis.",
    "references": [
      "https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/5-tools-and-sdks"
    ],
    "category": "Implement an agentic solution"
  },
  {
    "question_number": 115,
    "question_text": "You are building an app that uses Azure Translator in Foundry Tools document translation.\nYou need to improve the quality of the translation for user-uploaded documents.\nWhat should you ask the users to include when they upload a document?",
    "options": [
      {
        "letter": "a",
        "text": "a summary",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "the file format",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "the source language",
        "is_correct": true
      },
      {
        "letter": "d",
        "text": "the writing style",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "c"
    ],
    "explanation": "If the language of the content in the source document is known, it is recommended to specify the source language in the request to get a better translation.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/translator/document-translation/faq",
      "https://learn.microsoft.com/training/modules/translate-text-with-translator-service/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 116,
    "question_text": "You have an app that sends audio recordings from a call center to the speech-to-text feature of Azure Speech in Foundry Tools Service.\nDuring testing, you notice that the Word Error Rate (WER) is high and there are a lot of substitution errors.\nYou need to improve the model and reduce the WER.\nWhat should you add to the training data?",
    "options": [
      {
        "letter": "a",
        "text": "custom product and people names",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "overlapping speakers",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "people talking in the background",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Substitution errors are due to the model needing more training on custom product names and people names.\nOverlapping speakers define when there are more deletion errors. People talking in the background are detected when there are more insertion errors.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/speech-service/how-to-custom-speech-evaluate-data?pivots=speech-studio#resolve-errors-and-improve-wer",
      "https://learn.microsoft.com/training/modules/transcribe-speech-input-text/"
    ],
    "category": "Implement natural language processing solutions"
  },
  {
    "question_number": 117,
    "question_text": "You plan to build an app that will transcribe large quantities of audio files by using the Azure Speech in Foundry Tools service batch transcription feature.\nYou need to recommend a storage solution for the audio files. The solution must minimize development effort.\nWhat should you recommend?",
    "options": [
      {
        "letter": "a",
        "text": "Azure Cosmos DB",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "Azure Data Lake Storage",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "Azure SQL Database",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "Azure Storage",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d"
    ],
    "explanation": "Azure Storage is the only storage provider that can be used by default for batch transcription.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/speech-service/batch-transcription-create?pivots=speech-cli",
      "https://learn.microsoft.com/training/modules/transcribe-speech-input-text/"
    ],
    "category": "Implement generative AI solutions"
  },
  {
    "question_number": 118,
    "question_text": "You are building a chatbot that will use the Azure Language in Foundry Tools question answering service.\nYou need to identify the operational costs for the service.\nWhich two parameters will influence the costs? Each correct answer presents a complete solution.",
    "options": [
      {
        "letter": "a",
        "text": "the number of assigned metadata tags",
        "is_correct": false
      },
      {
        "letter": "b",
        "text": "the number of knowledge base editors",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "the number of supported languages",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "the required throughput",
        "is_correct": true
      },
      {
        "letter": "e",
        "text": "the size and the number of knowledge bases",
        "is_correct": true
      }
    ],
    "correct_answers": [
      "d",
      "e"
    ],
    "explanation": "The throughput, the size, and the number of knowledge bases affect the pricing tier, whereas the other parameters do not affect pricing.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/question-answering/concepts/azure-resources",
      "https://learn.microsoft.com/training/modules/build-qna-solution-qna-maker/"
    ],
    "category": "Plan and manage an Azure AI solution"
  },
  {
    "question_number": 119,
    "question_text": "You are building a solution that uses the Azure Language in Foundry Tools question answering service.\nYou need to import FAQ documents for the solution.\nWhich types of data will be extracted during the import process?",
    "options": [
      {
        "letter": "a",
        "text": "formatted text, URLs, and bulleted and numbered lists only",
        "is_correct": true
      },
      {
        "letter": "b",
        "text": "formatted text, URLs, images, and diagrams only",
        "is_correct": false
      },
      {
        "letter": "c",
        "text": "unformatted text, images, and diagrams only",
        "is_correct": false
      },
      {
        "letter": "d",
        "text": "unformatted text, numbered lists, and unstructured data only",
        "is_correct": false
      }
    ],
    "correct_answers": [
      "a"
    ],
    "explanation": "Currently, the extraction of images within documents that are uploaded to question answering for extraction is unsupported, as images need to be reachable via a public URL.",
    "references": [
      "https://learn.microsoft.com/azure/cognitive-services/language-service/question-answering/reference/document-format-guidelines",
      "https://learn.microsoft.com/training/modules/build-qna-solution-qna-maker/"
    ],
    "category": "Implement an agentic solution"
  }
]